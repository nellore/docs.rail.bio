{
    "docs": [
        {
            "location": "/", 
            "text": "Quick start\n\n\nInstallation\n tells you how to install Rail-RNA. You can then figure out how to use it by trial and error or try the \nTutorial\n. \nDeliverables\n describes Rail's various outputs and how to suppress ones you don't need to save time and money. \nReference\n covers command-line parameters across all of Rail's modes and job flows.\n\n\nWhat is Rail-RNA?\n\n\nRail-RNA is a spliced alignment program: it performs intron-aware alignment of RNA sequencing (RNA-seq) data to a genome assembly. You may have heard it's designed to run in the cloud with \nAmazon Web Services\n \nElastic MapReduce\n, and you may wonder why it exists since there are already many other spliced alignment programs, so we start with an\n\n\nAdvertisement\n\n\n\n\n\n\nRail-RNA doesn't just work in the cloud; it also works on computer clusters using batch schedulers like PBS or SGE via \nIPython Parallel\n and on single computers. If ever you see Rail described as cloud-based software, don't think, \"Then it's not for me because I'm never going to use a cloud computing service\":\n\n\n\n\n\n\nRail-RNA is for almost everyone working with RNA-seq data. We observe great performance on Illumina HiSeq/MiSeq data, aligning \n= 300-bp reads to human-size genomes. Our case is laid out in \nthis paper\n.\n\n\n\n\n\n\nNever say never. We've tried to make deploying Rail on Amazon Web Services easy enough to convince you to give the cloud a chance for analyzing a large RNA-seq dataset, and we're actively working on improving user experience. Think \nfreedom through the cloud\n rather than \nfreedom from the cloud\n: while the latter attitude has its merits, with the former, you won't need to compete for compute with other users of your institutional cluster, and the extent of your analyses won't be constrained by hard upper bounds on disk space and number of processing cores.\n\n\n\n\n\n\nDo you use \na streaming music service\n, \nNetflix\n, \nwebmail\n, or \nDropbox\n? Then you can thank cloud computing for reducing processing and storage burden on your local devices. Why wouldn't you try it out for your bioinformatics too? You already use the \nUCSC Genome Browser\n.\n\n\n\n\n\n\n\n\n\n\nRail-RNA is \n(mostly)\n MIT-licensed; you can and are encouraged to hack away at it. In fact, we've tried to make repackaging Rail really easy for you: you can \nroll your own installer\n. The meat of the source is a bunch of Python scripts, and you can view them \nhere\n. In particular, check out the scripts for the steps Rail-RNA executes \nhere\n). Each one has a long docstring explaining the formats of its input and output. Perhaps you'll want to mess with the step scripts to tweak outputs, or you'll submit us a \npull request\n to fix a bug. (Do that, please.) Our \npaper\n and especially its supplementary material explain in detail the algorithms used by each step.\n\n\n\n\n\n\nRail-RNA isn't just another aligner. While many spliced alignment programs output only SAM or BAM and perhaps a couple more text files summarizing intronic content for a single sample, Rail-RNA has several more outputs:\n\n\n\n\n\n\nbigWig\n files encoding coverage of the genome by reads in \ncoverage_bigwigs/\n. For each sample analyzed, there's one bigWig storing genome coverage by primary alignments and another storing genome coverage by uniquely aligning reads. There are also bigWigs with mean and median genome coverages across samples. To normalize each sample's contribution to these measures of center, the number of reads covering a given base of the genome is multiplied by some library size parameter (by default 40 million) and divided by the number of mapped reads in the sample. A bigWig file is an order of magnitude smaller than a BAM file---about 100 MB for 50 million reads. It can be used as input to several downstream analysis packages, including \nderfinder\n, \nedgeR-robust\n, and \nDESeq2\n. (See Leo Collado-Torres's explanation of how \nhere\n.) In fact, Rail's bigWig output is designed to work especially well with \nderfinder\n, which identifies differentially expressed regions of the genome among groups of samples. The Rail+\nderfinder\n pipeline is fully agnostic to gene annotation. Studying expression this way increases potential for discovery: in \nour Rail+\nderfinder\n reanalysis\n of the \nGEUVADIS dataset\n comprised of 667 paired-end lymphoblastoid cell line samples, 7.3% of expressed regions we uncovered occurred outside annotated genes.\n\n\n\n\n\n\nfeature matrices stored as gzipped TSV files (\ncross_sample_outputs/introns.tsv.gz\n, \ncross_sample_outputs/insertions.tsv.gz\n, and \ncross_sample_outputs/deletions.tsv.gz\n. The (i, j)th element of a given matrix is the number of reads in sample j in which evidence of feature i was found, where a feature is an intron, insertion, or deletion. A given feature matrix is straightforward to load in \nR\n with \nread.table\n.\n\n\n\n\n\n\na read count matrix \ncross_sample_outputs/read_counts.tsv\n. The (i, j)th element of this matrix contains the number of primary alignments of reads in sample i to chromosome j and the number of those alignments that are unique separated by a comma. Here, \"unique\" means that only one alignment of a given read was found to have the highest alignment score. The read count matrix is useful for normalization and required by \nderfinder\n. A supplementary normalization matrix \ncross_sample_outputs/normalization.tsv\n also provides \nupper-quartile normalization\n factors for the bigWig coverage vectors by sample.\n\n\n\n\n\n\nA \nBED\n file per sample per feature (introns, insertions, and deletions). The BED files are analogous to \nTopHat 2\n's BED output, in case you're used to it. Quotes below are from the \nTopHat 2 manual\n: \n\n\n\n\nintrons_and_indels/junctions.\nsample name\n.bed\n: \"Each junction consists of two connected BED blocks, where each block is as long as the maximal overhang of any read spanning the junction. The score is the number of alignments spanning the junction.\"\n\n\nintrons_and_indels/deletions.\nsample name\n.bed\n: \"chromLeft refers to the first genomic base of the deletion.\"\n\n\nintrons_and_indels/insertions.\nsample name\n.bed\n: \"chromLeft refers to the last genomic base before the insertion.\"\n\n\n\n\n\n\n\n\nYou can suppress any output type depending on your analysis goals, substantially reducing output size across many samples as well as the time Rail-RNA takes to run from end to end; see \nDeliverables\n. Indeed, since many RNA-seq analysis goals (e.g., differential expression) are achievable with just coverage information, it may not be necessary for you to ever work with a set of unwieldy BAM files.\n\n\n\n\n\n\nAs of v0.2, Rail-RNA can analyze dbGaP-protected RNA-seq data in the cloud in a manner compliant with \nNIH guidelines\n. Setting an Amazon Web Services account up for analyzing protected data with Rail-RNA should less than an hour using the protocol described in \ndbGaP on EMR\n.\n\n\n\n\n\n\nRail-RNA is designed to scale well with respect to computer cluster size and input data volume. It uses the \nMapReduce\n programming model to achieve this: it divides the alignment problem up into a sequence of alternating parallelizable aggregation and computation steps, where each is performed by a different Python script operating on a different input stream. (We use \nPyPy\n to improve performance.) For a single sample on single machine, it's about as fast as TopHat 2 on 8 processing cores, but it's about twice as fast as TopHat 2 on 32 cores. Rail-RNA also\n\n\n\n\n\n\neliminates redundant alignment work giving its throughput (measured in, for example, number of samples analyzed per hour) better-than-linear scaling with respect to the number of samples analyzed; and\n\n\n\n\n\n\nborrows strength across samples in the following ways.\n\n\n\n\n\n\nRail-RNA filters out out likely false positive junctions that appear in only a few samples and are covered by few reads across samples. (By default, junctions that appear in \n= 5% of samples and are covered by fewer than 5 reads in any one sample are filtered out.)\n\n\n\n\n\n\nReads in all samples are automatically realigned to transcript fragments (\"isofrags\") that overlap exon-exon junctions initially found in a subset of samples. This means it's likelier that exon-exon junctions covered by a small number of reads in any given sample will be discovered.\n\n\n\n\n\n\n\n\n\n\nIf you want fast and furious with few processing cores, try \nHISAT\n, \nSTAR\n, or \nSubjunc\n. If you want to perform an integrative analysis on over 20 similar RNA-seq samples---say, from the same tissue type---give Rail a shot. We're here to help if you have questions:\n\n\n\n\n\n\nGetting help\n\n\n\n\n\n\nRead the docs\n, but you're doing that right now.\n\n\n\n\n\n\nThere's a \nGitter chat room\n where we offer live support nontrivially often. You'll need an account with GitHub to get in. Don't be scared to talk! You're encouraged to use this resource.\n\n\n\n\n\n\nYou can always email \nAbhi Nellore\n with support questions. Put \n[Rail-RNA support]\n somewhere in the subject line, and include the version of Rail-RNA you used in your support request. To find this, enter \nrail-rna --version\n.\n\n\n\n\n\n\nUse the logs and error messages to figure out what went wrong yourself. Rail's source has verbose variable names, and it should be documented well enough. Since it's just some Python, you don't have to worry about compilation, either. Add try-except blocks and rerun part of your pipeline to find lines of input a script may be stumbling on. We may ask you to do things like this in the Gitter to troubleshoot, if you're willing.\n\n\n\n\n\n\nDisclaimer\n\n\nRenting Amazon Web Services resources costs money, regardless of whether your run ultimately succeeds or fails. In some cases, Rail-RNA or its documentation may be partially to blame for a failed run. While we are happy to review bug reports, we do not accept responsibility for financial damage caused by these errors. Rail-RNA is provided \"as is\" with no warranty.", 
            "title": "Home"
        }, 
        {
            "location": "/#quick-start", 
            "text": "Installation  tells you how to install Rail-RNA. You can then figure out how to use it by trial and error or try the  Tutorial .  Deliverables  describes Rail's various outputs and how to suppress ones you don't need to save time and money.  Reference  covers command-line parameters across all of Rail's modes and job flows.", 
            "title": "Quick start"
        }, 
        {
            "location": "/#what-is-rail-rna", 
            "text": "Rail-RNA is a spliced alignment program: it performs intron-aware alignment of RNA sequencing (RNA-seq) data to a genome assembly. You may have heard it's designed to run in the cloud with  Amazon Web Services   Elastic MapReduce , and you may wonder why it exists since there are already many other spliced alignment programs, so we start with an", 
            "title": "What is Rail-RNA?"
        }, 
        {
            "location": "/#advertisement", 
            "text": "Rail-RNA doesn't just work in the cloud; it also works on computer clusters using batch schedulers like PBS or SGE via  IPython Parallel  and on single computers. If ever you see Rail described as cloud-based software, don't think, \"Then it's not for me because I'm never going to use a cloud computing service\":    Rail-RNA is for almost everyone working with RNA-seq data. We observe great performance on Illumina HiSeq/MiSeq data, aligning  = 300-bp reads to human-size genomes. Our case is laid out in  this paper .    Never say never. We've tried to make deploying Rail on Amazon Web Services easy enough to convince you to give the cloud a chance for analyzing a large RNA-seq dataset, and we're actively working on improving user experience. Think  freedom through the cloud  rather than  freedom from the cloud : while the latter attitude has its merits, with the former, you won't need to compete for compute with other users of your institutional cluster, and the extent of your analyses won't be constrained by hard upper bounds on disk space and number of processing cores.    Do you use  a streaming music service ,  Netflix ,  webmail , or  Dropbox ? Then you can thank cloud computing for reducing processing and storage burden on your local devices. Why wouldn't you try it out for your bioinformatics too? You already use the  UCSC Genome Browser .      Rail-RNA is  (mostly)  MIT-licensed; you can and are encouraged to hack away at it. In fact, we've tried to make repackaging Rail really easy for you: you can  roll your own installer . The meat of the source is a bunch of Python scripts, and you can view them  here . In particular, check out the scripts for the steps Rail-RNA executes  here ). Each one has a long docstring explaining the formats of its input and output. Perhaps you'll want to mess with the step scripts to tweak outputs, or you'll submit us a  pull request  to fix a bug. (Do that, please.) Our  paper  and especially its supplementary material explain in detail the algorithms used by each step.    Rail-RNA isn't just another aligner. While many spliced alignment programs output only SAM or BAM and perhaps a couple more text files summarizing intronic content for a single sample, Rail-RNA has several more outputs:    bigWig  files encoding coverage of the genome by reads in  coverage_bigwigs/ . For each sample analyzed, there's one bigWig storing genome coverage by primary alignments and another storing genome coverage by uniquely aligning reads. There are also bigWigs with mean and median genome coverages across samples. To normalize each sample's contribution to these measures of center, the number of reads covering a given base of the genome is multiplied by some library size parameter (by default 40 million) and divided by the number of mapped reads in the sample. A bigWig file is an order of magnitude smaller than a BAM file---about 100 MB for 50 million reads. It can be used as input to several downstream analysis packages, including  derfinder ,  edgeR-robust , and  DESeq2 . (See Leo Collado-Torres's explanation of how  here .) In fact, Rail's bigWig output is designed to work especially well with  derfinder , which identifies differentially expressed regions of the genome among groups of samples. The Rail+ derfinder  pipeline is fully agnostic to gene annotation. Studying expression this way increases potential for discovery: in  our Rail+ derfinder  reanalysis  of the  GEUVADIS dataset  comprised of 667 paired-end lymphoblastoid cell line samples, 7.3% of expressed regions we uncovered occurred outside annotated genes.    feature matrices stored as gzipped TSV files ( cross_sample_outputs/introns.tsv.gz ,  cross_sample_outputs/insertions.tsv.gz , and  cross_sample_outputs/deletions.tsv.gz . The (i, j)th element of a given matrix is the number of reads in sample j in which evidence of feature i was found, where a feature is an intron, insertion, or deletion. A given feature matrix is straightforward to load in  R  with  read.table .    a read count matrix  cross_sample_outputs/read_counts.tsv . The (i, j)th element of this matrix contains the number of primary alignments of reads in sample i to chromosome j and the number of those alignments that are unique separated by a comma. Here, \"unique\" means that only one alignment of a given read was found to have the highest alignment score. The read count matrix is useful for normalization and required by  derfinder . A supplementary normalization matrix  cross_sample_outputs/normalization.tsv  also provides  upper-quartile normalization  factors for the bigWig coverage vectors by sample.    A  BED  file per sample per feature (introns, insertions, and deletions). The BED files are analogous to  TopHat 2 's BED output, in case you're used to it. Quotes below are from the  TopHat 2 manual :    introns_and_indels/junctions. sample name .bed : \"Each junction consists of two connected BED blocks, where each block is as long as the maximal overhang of any read spanning the junction. The score is the number of alignments spanning the junction.\"  introns_and_indels/deletions. sample name .bed : \"chromLeft refers to the first genomic base of the deletion.\"  introns_and_indels/insertions. sample name .bed : \"chromLeft refers to the last genomic base before the insertion.\"     You can suppress any output type depending on your analysis goals, substantially reducing output size across many samples as well as the time Rail-RNA takes to run from end to end; see  Deliverables . Indeed, since many RNA-seq analysis goals (e.g., differential expression) are achievable with just coverage information, it may not be necessary for you to ever work with a set of unwieldy BAM files.    As of v0.2, Rail-RNA can analyze dbGaP-protected RNA-seq data in the cloud in a manner compliant with  NIH guidelines . Setting an Amazon Web Services account up for analyzing protected data with Rail-RNA should less than an hour using the protocol described in  dbGaP on EMR .    Rail-RNA is designed to scale well with respect to computer cluster size and input data volume. It uses the  MapReduce  programming model to achieve this: it divides the alignment problem up into a sequence of alternating parallelizable aggregation and computation steps, where each is performed by a different Python script operating on a different input stream. (We use  PyPy  to improve performance.) For a single sample on single machine, it's about as fast as TopHat 2 on 8 processing cores, but it's about twice as fast as TopHat 2 on 32 cores. Rail-RNA also    eliminates redundant alignment work giving its throughput (measured in, for example, number of samples analyzed per hour) better-than-linear scaling with respect to the number of samples analyzed; and    borrows strength across samples in the following ways.    Rail-RNA filters out out likely false positive junctions that appear in only a few samples and are covered by few reads across samples. (By default, junctions that appear in  = 5% of samples and are covered by fewer than 5 reads in any one sample are filtered out.)    Reads in all samples are automatically realigned to transcript fragments (\"isofrags\") that overlap exon-exon junctions initially found in a subset of samples. This means it's likelier that exon-exon junctions covered by a small number of reads in any given sample will be discovered.      If you want fast and furious with few processing cores, try  HISAT ,  STAR , or  Subjunc . If you want to perform an integrative analysis on over 20 similar RNA-seq samples---say, from the same tissue type---give Rail a shot. We're here to help if you have questions:", 
            "title": "Advertisement"
        }, 
        {
            "location": "/#getting-help", 
            "text": "Read the docs , but you're doing that right now.    There's a  Gitter chat room  where we offer live support nontrivially often. You'll need an account with GitHub to get in. Don't be scared to talk! You're encouraged to use this resource.    You can always email  Abhi Nellore  with support questions. Put  [Rail-RNA support]  somewhere in the subject line, and include the version of Rail-RNA you used in your support request. To find this, enter  rail-rna --version .    Use the logs and error messages to figure out what went wrong yourself. Rail's source has verbose variable names, and it should be documented well enough. Since it's just some Python, you don't have to worry about compilation, either. Add try-except blocks and rerun part of your pipeline to find lines of input a script may be stumbling on. We may ask you to do things like this in the Gitter to troubleshoot, if you're willing.", 
            "title": "Getting help"
        }, 
        {
            "location": "/#disclaimer", 
            "text": "Renting Amazon Web Services resources costs money, regardless of whether your run ultimately succeeds or fails. In some cases, Rail-RNA or its documentation may be partially to blame for a failed run. While we are happy to review bug reports, we do not accept responsibility for financial damage caused by these errors. Rail-RNA is provided \"as is\" with no warranty.", 
            "title": "Disclaimer"
        }, 
        {
            "location": "/installation/", 
            "text": "Installing Rail-RNA\n\n\nMake sure you have a recent (\n= 2009) OS X or Linux box with at least 8 GB of RAM. For a no-fuss install, enter\n\n\n(INSTALLER=/var/tmp/$(cat /dev/urandom | env LC_CTYPE=C tr -cd 'a-f0-9' | head -c 32);\ncurl http://verve.webfactional.com/rail -o $INSTALLER; python2 $INSTALLER -m || true;\nrm -f $INSTALLER)\n\n\n\n\nin a Unix shell (which you open by running the \nTerminal\n app). Otherwise, download the latest version of Rail-RNA \nhere\n. If for some reason you need an older version \nV\n, you can visit \nthe \nreleases\n subdirectory\n of the repo, click on the file \ninstall-rail-rna-V\n, and click the \nRaw\n button to download it.\n\n\nThe file you download will be in the format \ninstall-rail-rna-V\n. In a Unix shell, navigate to the directory in which you downloaded the installer. This typically means entering \ncd ~/Downloads\n. Now enter \nchmod +x install-rail-rna-V\n to make the installer executable.\n\n\nOptions\n\n\nInstallation options may now be viewed by entering \n./install-rail-rna-V\n. They include\n\n\n\n\n\n\n-i/--install-dir \ndir\n: This is the directory in which Rail-RNA should be installed. The default is \"/usr/local/raildotbio\" if you install for all users, and \"~/raildotbio\" if you install for just the current user.\n\n\n\n\n\n\n-n/--no-dependencies\n: This installs Rail-RNA without any of its dependencies. Rail-RNA wraps \nBowtie 1\n, \nBowtie 2\n, \nSAMTools\n, \nbedGraphToBigWig\n, the \nAWS CLI\n (in \nelastic\n mode, to use Rail-RNA on Amazon Elastic MapReduce), and \nIPython Parallel\n (in \nparallel\n mode, for using Rail-RNA on more than one machine in various distributed computing environments). You can avoid installing them, and Rail-RNA will then look for the appropriate dependencies, but this is not recommended.\n\n\n\n\n\n\n-p/--prep-dependencies\n: This installs Rail-RNA with only dependencies required for its preprocess job flow. The option isn't that useful: Rail-RNA invokes it in a bootstrap script in \nelastic\n mode for job flows that preprocess input data. This avoids its having to install unnecessary dependencies on cloud computing clusters. The option is overrided by \n--no-dependencies\n.\n\n\n\n\n\n\n-y/--yes\n: Ordinarily, the user is asked several questions during installation. Invoking this option answers \"yes\" to all such questions, which means Rail-RNA is installed for all users, the default installation directory \n/usr/local/raildotbio\n is overwritten if it already exists, which means you upgrade, and the AWS CLI and IPython Parallel are installed if you don't have them already.\n\n\n\n\n\n\n-s/--symlink-dependencies\n You may have already installed versions of Rail's dependencies besides the AWS CLI and IPython Parallel, like SAMTools. By default, Rail-RNA does not change your system's default executables for these dependencies, and you can go on using this software the way you always did. However, if you are installing on a new system for all users and would like to expose Rail-RNA's dependencies, invoke this option to place symlinks to dependencies in /usr/local/bin.\n\n\n\n\n\n\n--curl \nexe\n: The installer downloads with \ncURL\n, which tends to come with Linux distributions and Mac OS. It's rarely necessary to specify the path to cURL directly, but that's what this option is for.\n\n\n\n\n\n\nYou can install Rail-RNA for all users or just for the current user. If you have root access and would like to install for all users, enter\n\n\nsudo ./install-rail-rna-V\n\n\n\n\n, enter your password, and proceed. To install for just you, enter\n\n\n./install-rail-rna-V\n\n\n\n\nFollow the prompts. Rail-RNA has several dependencies: \nBowtie 1\n, \nBowtie 2\n, \nSAMTools\n, and \nbedGraphToBigWig\n are the critical ones, and they are installed especially for Rail in \n~/raildotbio\n (single-user install) or \n/usr/local/raildotbio\n (all-user install) by default. (\n-d\n may be used to specify a different destination directory, as described above). You may have some of the critical dependencies already, but the installer will copy specific versions to the destination directory anyway to ensure reproducibility of results across installations of a given version of Rail-RNA. Don't worry: nothing happens to your original configuration, and the default versions of the dependencies you already have remain default---that is, unless you invoke the \n-s\n parameter described above. Optional dependencies are the \nAWS CLI\n, which you'll need to Rail-RNA on Amazon Elastic MapReduce, and \nIPython Parallel\n, which you'll need to run Rail-RNA over a conventional computer cluster. The installer will check to see if the optional dependencies are already present. If they're not, it will ask you if you want to install them.\n\n\nThe output of a full installation for all users looks like this.\n\n\ndhcp-pool-133:Downloads testuser$ sudo ./install_rail-rna-0.1.8b \n\u2200 Rail-RNA v0.1.8b Installer\nRail-RNA can be installed for all users or just the current user.\n    * Install for all users? [y/n]: y\nInstalled Rail-RNA.\nIPython is not installed but required for Rail-RNA to work in its\n\nparallel\n mode.\n    * Install IPython now? [y/n]: y\nAWS CLI is not installed but required for Rail-RNA to work in its\n\nelastic\n mode, on Amazon Elastic MapReduce.\n    * Install AWS CLI now? [y/n]: y\nInstallation log may be found at\n/usr/local/raildotbio/rail-rna_installer.log. Configure the\nAWS CLI by running \naws configure\n. Afterwards, run\n\naws emr create-default-roles\n to use default IAM roles on Amazon\nElastic MapReduce. To learn more about IAM roles, visit\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/roles-toplevel.html .\nStart using Rail by entering \nrail-rna\n.\n\n\n\n\nIf you plan to use Rail-RNA in its \nelastic\n mode, on Amazon Elastic MapReduce, you must additionally perform the steps in the following section.\n\n\nSetting up Amazon Elastic MapReduce\n\n\nFirst off, you need an account with Amazon Web Services (AWS). Sign up \nhere\n. If you're new to AWS, we highly recommend following \nthis\n tutorial by the \nGriffith Lab\n at \nWash U\n to get a feel for its use. But if you just want to set up an account, you need only read sections 4-6 contained there.\n\n\nIf you've just installed the AWS CLI along with Rail-RNA, you must configure it. Obtain your AWS Access Key ID and Secret Access Key by following the instructions \nhere\n.\n\n\nA note on security\n\n\nDon't put your credentials in a place where anyone else can find them, especially online. Hackers will \nsteal them\n and mine cryptocurrency on your dollar.\n If you accidentally put your credentials in a GitHub repo, \nscrub it\n and \nget new AWS keys\n immediately.\n\n\nConfiguring the AWS CLI\n\n\nNow enter\n\n\naws configure\n\n\n\n\nat the shell prompt. You will be prompted to enter your Access Key ID, your Secret Access Key, and your default region. \nus-east-1\n is the standard region for US customers, and it spans data centers on the East and West Coasts. However, if you or your input data live elsewhere, you may want to default to one of the other regions listed \nhere\n. For example, we find that Rail is fastest at downloading and preprocessing data hosted by the \nEuropean Nucleotide Archive\n in the \neu-west-1\n region, which is in Ireland.\n\n\nCreating default roles\n\n\nYou must also set up \nIAM\n roles for use with Elastic MapReduce. A discussion of IAM roles for Elastic MapReduce is available \nhere\n. IAM is the way AWS securely gives users and services permission to access resources. The default set of permissions tends to work fine; if they don't for you, then you probably know enough about IAM to set up roles for Elastic MapReduce yourself. The typical way to get setting up roles over with is by entering\n\n\naws emr create-default-roles\n\n\n\n\n, but it's possible you're managing a lab whose members are IAM users attached to your AWS account, and you've already given them permissions. In this case, you will need to make sure your lab members have the \niam:GetInstanceProfile\n, \niam:GetRole\n, and \niam:PassRole\n permissions. We also recommend you give them the \niam:ListRoles\n permission; otherwise, they won't be able clone clusters on Elastic MapReduce. This is sometimes useful if their job flows fail because their bid prices on the \nspot market\n were too low, and they want to restart job flows easily. With the appropriate permissions, lab members can install Rail-RNA and configure the AWS CLI as described above. This includes their running \naws emr create-default-roles\n, which will retrieve the default roles you created for them. Learn more about working with IAM users \nhere\n. This step, and if you find yourself having trouble, ask for help in our \nGitter\n.\n\n\nFor hackers\n\n\nHow the installer works\n\n\nThe Rail-RNA installer is nothing but a ZIP archive. When it is executed by Python while it's still compressed, the installer is run. If you unpack the archive first and then execute the directory containing \n__main__.py\n, Rail-RNA is run, and it will complain if it can't find dependencies. If you're a Python developer who needs to write a custom installer because your software is difficult to package with something like \npip\n, you might want to try this approach. A helpful starting point is \nthis page\n.\n\n\nRolling your own installer\n\n\nWe've made it easy for you to release your own custom version of Rail-RNA.\n\n\n\n\n\n\nClone the source at a shell prompt with\n\n\ngit clone https://www.github.com/nellore/rail.git\n\n\n\n. We assume you've cloned to \n/home/testuser/rail\n below.\n\n\n\n\n\n\nEdit \nsrc/version.py\n, which looks like this:\n\n\n!/usr/bin/env python\n\"\"\"\nversion.py\nPart of Rail-RNA\n\nStores version number of Rail-RNA as a string.\n\"\"\"\n\nversion_number = 'devel'\n\n\n\nChange \"devel\" to some version number that diverges from the Rail-RNA versioning scheme. Perhaps you'll use a prefix \"C\" for \"custom,\" as in \"C0.1.0\". We call your custom version C below.\n\n\n\n\n\n\nEdit source files in \nsrc/\n however you want. You'll probably want to focus on the files in \nsrc/rna/steps\n, which contains a script per step of the Rail-RNA pipeline. To test your changes, rather than starting a command with \nrail-rna\n, use\n        python /home/testuser/rail/src\n.\n\n\n\n\n\n\nOnce you're done hacking Rail, run\n\n\nsh /home/testuser/rail/make_it_rail.sh\n\n\n\nA new installer \ninstall-rail-rna-C\n will appear in the \nreleases/\n directory. Run it to install your modified version locally.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#installing-rail-rna", 
            "text": "Make sure you have a recent ( = 2009) OS X or Linux box with at least 8 GB of RAM. For a no-fuss install, enter  (INSTALLER=/var/tmp/$(cat /dev/urandom | env LC_CTYPE=C tr -cd 'a-f0-9' | head -c 32);\ncurl http://verve.webfactional.com/rail -o $INSTALLER; python2 $INSTALLER -m || true;\nrm -f $INSTALLER)  in a Unix shell (which you open by running the  Terminal  app). Otherwise, download the latest version of Rail-RNA  here . If for some reason you need an older version  V , you can visit  the  releases  subdirectory  of the repo, click on the file  install-rail-rna-V , and click the  Raw  button to download it.  The file you download will be in the format  install-rail-rna-V . In a Unix shell, navigate to the directory in which you downloaded the installer. This typically means entering  cd ~/Downloads . Now enter  chmod +x install-rail-rna-V  to make the installer executable.  Options  Installation options may now be viewed by entering  ./install-rail-rna-V . They include    -i/--install-dir  dir : This is the directory in which Rail-RNA should be installed. The default is \"/usr/local/raildotbio\" if you install for all users, and \"~/raildotbio\" if you install for just the current user.    -n/--no-dependencies : This installs Rail-RNA without any of its dependencies. Rail-RNA wraps  Bowtie 1 ,  Bowtie 2 ,  SAMTools ,  bedGraphToBigWig , the  AWS CLI  (in  elastic  mode, to use Rail-RNA on Amazon Elastic MapReduce), and  IPython Parallel  (in  parallel  mode, for using Rail-RNA on more than one machine in various distributed computing environments). You can avoid installing them, and Rail-RNA will then look for the appropriate dependencies, but this is not recommended.    -p/--prep-dependencies : This installs Rail-RNA with only dependencies required for its preprocess job flow. The option isn't that useful: Rail-RNA invokes it in a bootstrap script in  elastic  mode for job flows that preprocess input data. This avoids its having to install unnecessary dependencies on cloud computing clusters. The option is overrided by  --no-dependencies .    -y/--yes : Ordinarily, the user is asked several questions during installation. Invoking this option answers \"yes\" to all such questions, which means Rail-RNA is installed for all users, the default installation directory  /usr/local/raildotbio  is overwritten if it already exists, which means you upgrade, and the AWS CLI and IPython Parallel are installed if you don't have them already.    -s/--symlink-dependencies  You may have already installed versions of Rail's dependencies besides the AWS CLI and IPython Parallel, like SAMTools. By default, Rail-RNA does not change your system's default executables for these dependencies, and you can go on using this software the way you always did. However, if you are installing on a new system for all users and would like to expose Rail-RNA's dependencies, invoke this option to place symlinks to dependencies in /usr/local/bin.    --curl  exe : The installer downloads with  cURL , which tends to come with Linux distributions and Mac OS. It's rarely necessary to specify the path to cURL directly, but that's what this option is for.    You can install Rail-RNA for all users or just for the current user. If you have root access and would like to install for all users, enter  sudo ./install-rail-rna-V  , enter your password, and proceed. To install for just you, enter  ./install-rail-rna-V  Follow the prompts. Rail-RNA has several dependencies:  Bowtie 1 ,  Bowtie 2 ,  SAMTools , and  bedGraphToBigWig  are the critical ones, and they are installed especially for Rail in  ~/raildotbio  (single-user install) or  /usr/local/raildotbio  (all-user install) by default. ( -d  may be used to specify a different destination directory, as described above). You may have some of the critical dependencies already, but the installer will copy specific versions to the destination directory anyway to ensure reproducibility of results across installations of a given version of Rail-RNA. Don't worry: nothing happens to your original configuration, and the default versions of the dependencies you already have remain default---that is, unless you invoke the  -s  parameter described above. Optional dependencies are the  AWS CLI , which you'll need to Rail-RNA on Amazon Elastic MapReduce, and  IPython Parallel , which you'll need to run Rail-RNA over a conventional computer cluster. The installer will check to see if the optional dependencies are already present. If they're not, it will ask you if you want to install them.  The output of a full installation for all users looks like this.  dhcp-pool-133:Downloads testuser$ sudo ./install_rail-rna-0.1.8b \n\u2200 Rail-RNA v0.1.8b Installer\nRail-RNA can be installed for all users or just the current user.\n    * Install for all users? [y/n]: y\nInstalled Rail-RNA.\nIPython is not installed but required for Rail-RNA to work in its parallel  mode.\n    * Install IPython now? [y/n]: y\nAWS CLI is not installed but required for Rail-RNA to work in its elastic  mode, on Amazon Elastic MapReduce.\n    * Install AWS CLI now? [y/n]: y\nInstallation log may be found at\n/usr/local/raildotbio/rail-rna_installer.log. Configure the\nAWS CLI by running  aws configure . Afterwards, run aws emr create-default-roles  to use default IAM roles on Amazon\nElastic MapReduce. To learn more about IAM roles, visit\nhttp://docs.aws.amazon.com/IAM/latest/UserGuide/roles-toplevel.html .\nStart using Rail by entering  rail-rna .  If you plan to use Rail-RNA in its  elastic  mode, on Amazon Elastic MapReduce, you must additionally perform the steps in the following section.", 
            "title": "Installing Rail-RNA"
        }, 
        {
            "location": "/installation/#setting-up-amazon-elastic-mapreduce", 
            "text": "First off, you need an account with Amazon Web Services (AWS). Sign up  here . If you're new to AWS, we highly recommend following  this  tutorial by the  Griffith Lab  at  Wash U  to get a feel for its use. But if you just want to set up an account, you need only read sections 4-6 contained there.  If you've just installed the AWS CLI along with Rail-RNA, you must configure it. Obtain your AWS Access Key ID and Secret Access Key by following the instructions  here .  A note on security  Don't put your credentials in a place where anyone else can find them, especially online. Hackers will  steal them  and mine cryptocurrency on your dollar.  If you accidentally put your credentials in a GitHub repo,  scrub it  and  get new AWS keys  immediately.  Configuring the AWS CLI  Now enter  aws configure  at the shell prompt. You will be prompted to enter your Access Key ID, your Secret Access Key, and your default region.  us-east-1  is the standard region for US customers, and it spans data centers on the East and West Coasts. However, if you or your input data live elsewhere, you may want to default to one of the other regions listed  here . For example, we find that Rail is fastest at downloading and preprocessing data hosted by the  European Nucleotide Archive  in the  eu-west-1  region, which is in Ireland.  Creating default roles  You must also set up  IAM  roles for use with Elastic MapReduce. A discussion of IAM roles for Elastic MapReduce is available  here . IAM is the way AWS securely gives users and services permission to access resources. The default set of permissions tends to work fine; if they don't for you, then you probably know enough about IAM to set up roles for Elastic MapReduce yourself. The typical way to get setting up roles over with is by entering  aws emr create-default-roles  , but it's possible you're managing a lab whose members are IAM users attached to your AWS account, and you've already given them permissions. In this case, you will need to make sure your lab members have the  iam:GetInstanceProfile ,  iam:GetRole , and  iam:PassRole  permissions. We also recommend you give them the  iam:ListRoles  permission; otherwise, they won't be able clone clusters on Elastic MapReduce. This is sometimes useful if their job flows fail because their bid prices on the  spot market  were too low, and they want to restart job flows easily. With the appropriate permissions, lab members can install Rail-RNA and configure the AWS CLI as described above. This includes their running  aws emr create-default-roles , which will retrieve the default roles you created for them. Learn more about working with IAM users  here . This step, and if you find yourself having trouble, ask for help in our  Gitter .", 
            "title": "Setting up Amazon Elastic MapReduce"
        }, 
        {
            "location": "/installation/#for-hackers", 
            "text": "How the installer works  The Rail-RNA installer is nothing but a ZIP archive. When it is executed by Python while it's still compressed, the installer is run. If you unpack the archive first and then execute the directory containing  __main__.py , Rail-RNA is run, and it will complain if it can't find dependencies. If you're a Python developer who needs to write a custom installer because your software is difficult to package with something like  pip , you might want to try this approach. A helpful starting point is  this page .  Rolling your own installer  We've made it easy for you to release your own custom version of Rail-RNA.    Clone the source at a shell prompt with  git clone https://www.github.com/nellore/rail.git  . We assume you've cloned to  /home/testuser/rail  below.    Edit  src/version.py , which looks like this:  !/usr/bin/env python\n\"\"\"\nversion.py\nPart of Rail-RNA\n\nStores version number of Rail-RNA as a string.\n\"\"\"\n\nversion_number = 'devel'  Change \"devel\" to some version number that diverges from the Rail-RNA versioning scheme. Perhaps you'll use a prefix \"C\" for \"custom,\" as in \"C0.1.0\". We call your custom version C below.    Edit source files in  src/  however you want. You'll probably want to focus on the files in  src/rna/steps , which contains a script per step of the Rail-RNA pipeline. To test your changes, rather than starting a command with  rail-rna , use\n        python /home/testuser/rail/src\n.    Once you're done hacking Rail, run  sh /home/testuser/rail/make_it_rail.sh  A new installer  install-rail-rna-C  will appear in the  releases/  directory. Run it to install your modified version locally.", 
            "title": "For hackers"
        }, 
        {
            "location": "/tutorial/", 
            "text": "All examples in this tutorial were generated by \nFlux Simulator\n with \nthis\n script from the Rail repo.\n\n\nTrial and error\n\n\nWe made a serious attempt to design Rail-RNA for people who don't read manuals to discourage improper use of the software. In general, you can get started by entering\n\n\nrail-rna\n\n\n\n\nin the shell. You'll obtain the following output, perhaps with a different version number.\n\n\nerror: too few arguments\nusage: rail-rna \njob flow\n \nmode\n \n[args]\n\n\n  \njob flow\n       {prep, align, go}\n                     prep: preprocess reads listed in a required manifest\n                       file (specified with --manifest)\n                     align: align preprocessed reads (specified with --input)\n                     go: perform prep and align in succession\n  \nmode\n           {local, parallel, elastic}\n                     local: run Rail-RNA on this computer\n                     parallel: run Rail-RNA on all active IPython engines\n                     elastic: run Rail-RNA on Amazon Elastic MapReduce.\n                       Requires that the user sign up for Amazon Web Services\n\n\u2200 Rail-RNA v0.2.0 by Abhi Nellore (anellore@jhu.edu; nellore.github.io)\n\nRail-RNA is a scalable MapReduce pipeline that can analyze many RNA-seq\ndatasets at once. To view help for a given combination of \njob flow\n and\n\nmode\n, specify both, then add -h/--help.\n\n\n\n\nIt's quite possible to figure out how a substantial fraction Rail-RNA works by trial and error. Follow the prompts and fix your errors, which Rail tries to explain in detail when they're encountered. Please let us know in the \nGitter\n if you're confused by any errors output. If you prefer to be guided through some examples, read on.\n\n\nModes and job flows\n\n\nRail-RNA has three modes---\n\n\n\n\n\n\nlocal\n, which runs the software on a single machine on up to as many processing cores as available. By default, Rail-RNA runs on the number of processing cores it detects on a machine less one to avoid monopolizing a single machine's resources. To adjust the number of cores on which Rail is run, use the \n-p/--num-processes\n command-line parameter described under Reference.\n\n\n\n\n\n\nelastic\n, which runs the software on an Amazon Elastic MapReduce computer cluster in the cloud. This costs money: you rent computing capacity (on \nElastic Compute Cloud\n, or EC2) and storage (on \nSimple Storage Service\n, or S3) from Amazon for the duration of your job flow and for as long as you keep your outputs in the cloud. Price information is \nhere\n. For a medium-size job (up to ~100 RNA-seq samples with ~50 million reads each), we tend to use a cluster of 40 c3.2xlarge machines, which costs $21/hour on demand but perhaps less than half that number on the spot market in the US Standard zone (\nus-east-1\n). Learn more about the spot market and bidding on compute capacity \nhere\n.\n\n\n\n\n\n\nparallel\n, which runs on an IPython Parallel cluster whose profile is specified at the command line. You must set up the IPython Parallel cluster yourself.\n\n\n\n\n\n\n---and three job flows---\n\n\n\n\n\n\nprep\n, which if necessary downloads input FASTQ/FASTA files from a remote server and casts it in a form amenable for further analysis. This job flow must always be run before the \nalign\n job flow.\n\n\n\n\n\n\nalign\n, which aligns preprocessed data and writes outputs explained in \nDeliverables\n. Use the \n--deliverables\n option described there to control which of Rail-RNA's terminal outputs should be written.\n\n\n\n\n\n\ngo\n, which runs the \nprep\n and \nalign\n job flows in succession.\n\n\n\n\n\n\nTo run some combination of mode and job flow, start your command with\n\n\nrail-rna \nmode\n \njob flow\n\n\n\n\n\n. Get help by entering\n\n\nrail-rna \nmode\n \njob flow\n -h\n\n\n\n\nThe reason for separating the \nprep\n and \nalign\n job flows is that you may prefer to run these flows on different numbers of processing cores: if downloads are throttled by the remote server, and only a small number of threads can download input files concurrently, then you can limit the size of the computer cluster in \nelastic\n mode (with the \n-c/--core-instance-count\n parameter described in Reference) or the number of concurrently running threads in \nlocal\n mode (with the \n-p/--num-processes\n parameter described in Reference).\n\n\nAlign some RNA-seq samples with us in \nlocal\n, \nparallel\n, and \nelastic\n modes below. We assume your login name is \ntestuser\n, and your home directory is \n/home/testuser\n.\n\n\nlocal\n and \nparallel\n modes: Drosophila examples\n\n\nRail-RNA wraps both \nBowtie 1\n and \nBowtie 2\n, so you will need \nboth\n Bowtie 1 and Bowtie 2 indexes of a genome assembly to perform alignment. It's easiest to download one of the \nIllumina iGenomes\n, collections of reference sequences and indexes for popularly studied organisms. Here are explicit instructions:\n\n\nDownload the Drosophila melanogaster build dm3 iGenome, available \nhere\n. This is a 783-MB file. We assume you downloaded the dm3 iGenome to the directory \n/home/testuser/Downloads\n. Unpack it by entering\n\n\ncd /home/testuser/Downloads\ntar xvzf Drosophila_melanogaster_UCSC_dm3.tar.gz\n\n\n\n\nin the shell. This could take a while, but once it's done, the Bowtie 1 index basename should be \n/home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome\n, while the Bowtie 2 index basename should be \n/home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome\n.\n\n\nWarning:\n You may think you already have Bowtie 1 and 2 indexes for a given genome assembly---perhaps dm3---but the contigs in one may not be exactly the same as the contigs in the other. When working with any reference in Rail-RNA, it is important to ensure that the Bowtie 1 and 2 indexes are built from the same FASTA. Check the \nBowtie 1 documentation\n and \nBowtie 2 documentation\n for information on how to build Bowtie indexes from reference FASTAs.\n\n\nNow open your browser, and navigate to \nthis URL\n. You'll see the following text.\n\n\nhttp://verve.webfactional.com/dm3_example_1_left.fastq       0       http://verve.webfactional.com/dm3_example_1_right.fastq        0       dm3_example-1-1\nhttp://verve.webfactional.com/dm3_example_2_left.fastq       0       http://verve.webfactional.com/dm3_example_2_right.fastq        0       dm3_example-2-1\n\n\n\n\nThis is a Rail-RNA manifest file, and it is required for all Rail-RNA runs. Its format mirrors the manifest file format of \nMyrna\n, a predecessor of Rail. Each line corresponds to a different RNA-seq sample. A line for a single-end sample looks like this---\n\n\nFASTQ/FASTA URL\n(tab)\nURL MD5 checksum or 0\n(tab)\nsample label\n\n\n\n\n\n---while a line for a paired-end sample looks like this---\n\n\nFASTQ/FASTA URL 1\n(tab)\nURL 1 MD5 checksum or 0\n(tab)\nFASTQ/FASTA URL 2\n(tab)\nURL 2 MD5 checksum or 0\n(tab)\nsample label\n\n\n\n\n\n. URLs can be on the local filesystem, on the web, or if the \nAWS CLI\n is installed, on Amazon S3. If an input file is not on the local filesystem, it will be downloaded by Rail. Manifest files may be hosted remotely as well.\n\n\nOur Drosophila example is composed of two paired-end biological replicates, and the raw data is hosted at \nhttp://verve.webfactional.com\n . Let's create a new \nrailtests\n directory in \ntestuser\n's home directory for running all our examples. \n\n\nmkdir -p /home/testuser/railtests\ncd /home/testuser/railtests\n\n\n\n\nPreprocessing and aligning the test data in \nlocal\n mode takes a single command:\n\n\nrail-rna go local -x /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome -m https://raw.githubusercontent.com/nellore/rail/master/ex/dm3_example.manifest\n\n\n\n\n. The run should complete after a few minutes. You can browse the output by entering\n\n\ncd /home/testuser/railtests/rail-rna_out\nls\n\n\n\n\nYou'll find four directories:\n\n\nalignments    cross_sample_results\ncoverage_bigwigs  junctions_and_indels\n\n\n\n\nSee \nDeliverables\n for information on how to interpret what's in these directories.\n\n\nCommonly used options in \nlocal\n mode are\n\n \n-p/--num-processes\n, which controls the number of processes Rail-RNA runs simultaneously. By default, Rail uses as many processing cores as your computer has less one so it doesn't monopolize resources. You may want to run on all cylinders by setting this equal to the number of available processing cores.\n\n \n-o/--output\n, which changes the output directory. By default, this is \nrail-rna_out\n in the current directory.\n\n\nSee the Reference section for a comprehensive description of command-line parameters.\n\n\nWe could have run Rail-RNA in two steps: by preprocessing first and subsequently aligning. You can try this yourself by running the following commands in sequence.\n\n\ncd /home/testuser/railtests\nrail-rna prep local -m https://raw.githubusercontent.com/nellore/rail/master/ex/dm3_example.manifest -o dmel_prepped -p 1\nrail-rna align local -i dmel_prepped -m https://raw.githubusercontent.com/nellore/rail/master/ex/dm3_example.manifest -x home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome -f\n\n\n\n\nThe \nrail-rna prep local\n command above is executed on a single thread (with \n-p 1\n) to ensure that only one download is performed at a time, which would be useful if our downloads were throttled. In the \nrail-rna align local\n command, the parameter \n-f/--force\n tells Rail to overwrite any existing outputs. You should end up with exactly the same output you obtained when you ran the \nrail-rna go local\n command.\n\n\nNow you'll run an example that will break Rail. Navigate to \nthis URL\n, a manifest file that should look like this:\n\n\nhttp://verve.webfactional.com/bad.fastq       0        bad-1-1\n\n\n\n\nIt's a single-end sample called \"bad,\" presumably because it's not good. Visit http://verve.webfactional.com/bad.fastq next. The file will be downloaded, or it will be displayed in your browser. If it's downloaded, navigate to the appropriate folder at the shell prompt, and enter\n\n\ncat bad.fastq\n\n\n\n\n. Either way, you'll see\n\n\n@badrecord\nATACAGATGACAGATGACAGGGTAGAGACAAATAGACAGATGACGATGGACAGATGACAGATAGAACAGATAGAGA\n+\nIIIIIIIIIIIIII\n@goodrecord\nATGGCATCAGTCAAGTCAAGATTACTAGTAGCCATACAAGATACATCGTTTAACGATTGTGGCACATACGTCACCA\n+\nIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII\n\n\n\n\nThere's one sequence labeled \"badrecord\" and another labeled \"goodrecord\". \"badrecord\" is bad because its read sequence isn't the same length as its quality sequence. Without being told otherwise, Rail-RNA chokes on bad records, but it's good to see how it chokes to learn how to diagnose problems. Run Rail-RNA on the bad manifest file like so---\n\n\ncd /home/testuser/railtests\nrail-rna go local -x /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome -m https://raw.githubusercontent.com/nellore/rail/master/ex/bad.manifest -o ./bad\n\n\n\n\n---and you'll ultimately obtain output like this:\n\n\n\u2200 Rail-RNA v0.2.0\nStarted job flow on Saturday, Jul 18, 2015 at 07:36:29 PM EDT.\n\n~.oOo.\n\n\n00h:00m:00s |___| Step 1/24: Count lines in input files\n00h:00m:01s |___|     Completed 1 task.\n00h:00m:01s |___|     Deleted temporary files.\n00h:00m:01s |___| Step 2/24: Assign reads to preprocessing tasks\n00h:00m:01s |___|     Partitioned 1 input into tasks.\n00h:00m:03s |___|     Completed 1 task.\n00h:00m:03s |___|     Deleted temporary files.\n00h:00m:03s |___| Step 3/24: Preprocess reads\n*****Errors encountered*****\nStreaming command \ncat /var/folders/rs/zytvc1753p9_bth4lrhbm7c80000gn/T/tmpCxbBKB/0 | /usr/local/raildotbio/pypy-2.5.0-osx64/bin/pypy /usr/local/raildotbio/rail-rna/rna/steps/preprocess.py --nucs-per-file=100000000 --gzip-output --push=/home/testuser/railtests/rail-rna_logs/preprocess/push --gzip-level 3   --bin-qualities \n/home/testuser/railtests/rail-rna_logs/preprocess/0 2\n/home/testuser/railtests/rail-rna_logs/preprocess/dp.map.log/0.0.log\n failed; exit level was 1.\nJob flow failed on Saturday, Jul 18, 2015 at 07:36:34 PM EDT. Run time was 5.503 seconds.\nTo start this job flow from where it left off, run:\n/usr/local/raildotbio/pypy-2.5.0-osx64/bin/pypy /usr/local/raildotbio/rail-rna/dooplicity/emr_simulator.py -j /home/testuser/railtests/rail-rna_logs/resume_flow_WDEZ24GVC0TM.json -b /usr/local/raildotbio/rail-rna/rna/driver/rail-rna.txt -l /home/testuser/railtests/rail-rna_logs/flow.2015-07-18T19:36:27.934937.log -f --max-attempts 1 --num-processes 3\nTraceback (most recent call last):\n  File \napp_main.py\n, line 75, in run_toplevel\n  File \n/usr/local/raildotbio/rail-rna/dooplicity/emr_simulator.py\n, line 1963, in \nmodule\n\n    args.scratch, args.common, args.sort, args.max_attempts)\n  File \n/usr/local/raildotbio/rail-rna/dooplicity/emr_simulator.py\n, line 1719, in run_simulation\n    max_attempts=max_attempts\n  File \n/usr/local/raildotbio/rail-rna/dooplicity/emr_simulator.py\n, line 1290, in execute_balanced_job_with_retries\n    raise RuntimeError\nRuntimeError\n\n\n\n\nRail is telling us that something went wrong with a command during preprocessing. Note the following.\n\n\n\n\n\n\nRail gives you a command you can use to resume a job flow if it's failed for a fixable reason. One possible reason is that you ran out of space on disk during a job flow, and you need to delete some of your files. You can highlight and copy the command that resumes your job flow with \nCommand+C\n on a Mac or \nCTRL+C\n in Linux. If somehow you lose the resume command, you'll also find it in the last written file whose extension is \n.log\n in the log directory \n/home/testuser/railtests/rail-rna_logs\n. Enter \n\n\nls -tr /home/testuser/railtests/rail-rna_logs/*.log | tail -n 1\n\n\n\nto get its path.\n\n\n\n\n\n\nThere's a log file in the command that failed, right after the \n2\n. This file will tell you why the job flow failed. Open it with \nless\n:\n\n\nless /home/testuser/dmel/rail-rna_logs/preprocess/dp.map.log/0.0.log\n\n\n\n. (You should \nless\n whatever log file appears for you after the \n2\n). At the bottom of the file is the exception that made the job flow fail:\n\n\nCreated local destination directory \"/var/folders/rs/zytvc1753p9_bth4lrhbm7c80000gn/T/tmpQJHduB\".\nRetrieving URL \"http://verve.webfactional.com/bad.fastq\"...\nRange of quality values found from random sample of 10000 records is (73, 73).\nGuessed Phred64 encoding.\nTraceback (most recent call last):\n  File \"app_main.py\", line 75, in run_toplevel\n  File \"/usr/local/raildotbio/rail-rna/rna/steps/preprocess.py\", line 863, in \nmodule\n\n    mover=mover)\n  File \"/usr/local/raildotbio/rail-rna/rna/steps/preprocess.py\", line 557, in go\n    ) % (line_numbers[i], sources[i])\nAssertionError: Length of read sequence does not match length of quality string at line 4 of file \"/var/folders/rs/zytvc1753p9_bth4lrhbm7c80000gn/T/tmpQJHduB/bad.fastq\".\n\n\n\n. So Rail noticed \n@badrecord\n was a bad record and failed. You can have Rail ignore bad records during preprocessing using the \n--ignore-bad-records\n command-line parameter:\n\n\nrail-rna go local -x /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome -m https://raw.githubusercontent.com/nellore/rail/master/ex/bad.manifest -o ./bad -f --ignore-bad-records\n\n\n\n. This job flow should succeed, and you'll end up with exactly one aligned read in the directory \n/home/testuser/railtests/bad/alignments\n. Ignoring bad records may be useful when you're analyzing hundreds of samples and, for example, one file is truncated, but you'd like to analyze all its available reads anyway. Sometimes, you can't anticipate the integrity of the data you're analyzing, but you don't want your job flow to fail because of a handful of bad records.\n\n\n\n\n\n\nTo test Rail-RNA in \nparallel\n mode, you should have IPython installed. If the Rail-RNA installer didn't prompt you to install it, you already have it. Detailed instructions on starting IPython clusters over many different cluster configurations may be found \nhere\n. The IPython documentation around this link teaches you how to create a profile for a cluster configuration and how to start a cluster. An IPython cluster is nothing but a collection of Python interpreters (\"engines\") running on processing cores distributed across some networked computers. Rail-RNA does not start an IPython cluster for you; rather, it detects a running IPython cluster and runs itself over that. You can choose the IPython profile Rail-RNA should use with the \n--ipython-profile\n command-line parameter. If left unspecified, this is taken to be the default profile.\n\n\nFor testing purposes, you can run an IPython cluster on just your machine by entering\n\n\nipcluster start -n \nk\n\n\n\n\n\n, where \nk\n is the number of IPython engine processes you want to run. \nk\n is analogous to the \n-p/--num-processes\n parameter of Rail-RNA in local mode. Now rerun the first Drosophila example except in \nparallel\n mode by entering\n\n\nrail-rna go parallel -x /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome -m https://raw.githubusercontent.com/nellore/rail/master/ex/dm3_example.manifest -f\n\n\n\n\nWhat happens is pretty unimpressive: Rail runs like it does in local mode. But IPython Parallel lends Rail versatility: there are many different IPython cluster configurations you can set up on, say, your institutional computer cluster. But there is one caveat: in \nparallel\n mode, you \nmust\n make sure the output directory (specified with \n-o/--output\n) and the log directory (specified with \n--log\n) are at paths accessible to \nall\n the nodes in your cluster running IPython engines. Rail-RNA aggregates intermediate data in the log directory between any two successive steps, and all nodes must be able to stream data from that directory to complete their assigned tasks. Moreover, outputs must go to the same place, and some paths on a cluster refer to different places on different nodes. For example, you probably shouldn't make a subdirectory of \n/tmp\n your output or log directory on a conventional computer cluster because \n/tmp\n tends to be a node-local temporary directory.\n\n\nelastic\n mode: a human example\n\n\nWarning: running this example will cost money\n, but probably no more than US$5. Read our \ndisclaimer\n before using \nelastic\n mode.\n\n\nYou should have performed \nthese\n steps to set up the AWS CLI and Elastic MapReduce before attempting the example.\n\n\nCheck out \nthis manifest file\n in your browser. Listed are two single-end human samples with just 20,000 reads each. They were generated with expression profiles of two \nGEUVADIS\n lymphoblastoid cell line samples in a way we describe in our \npaper\n, but because there are so few reads, you probably couldn't tell which GEUVADIS samples we used if you didn't have the sample labels in the manifest file.\n\n\nLet's use Elastic MapReduce to both preprocess and align these data. Since you'll be storing the results on Amazon's \nSimple Storage Service\n (S3), you should first read up on \nworking with buckets\n. The short story is that on S3, a bucket is something like a directory in a filesystem, except\n\n\n\n\nUnless you specify otherwise, only your account can access it.\n\n\nBucket names are globally unique: if someone else has taken a bucket name in any Amazon region, you can't have it.\n\n\nUnderscores in bucket names are a bad idea when you use Elastic MapReduce. They cause weird problems. Don't ask why; just go with it.\n\n\nA bucket is located in a specific Amazon region---that is, a bucket has a physical location in an Amazon data center, and that's where your files go.\n\n\n\n\nYou can create a bucket at the command line using the AWS CLI. Enter\n\n\naws s3 mb s3://this-is-the-bucket-name-you-make-up --region \na valid region\n\n\n\n\n\n. A list of valid regions is available \nhere\n. Since you specified your default region when configuring the AWS CLI, if you leave the \n--region\n part out above, the bucket will be created in your default region---typically \nus-east-1\n.\n\n\nBut you don't \nhave\n to create a bucket yourself before using Rail; if it doesn't already exist, Rail will automatically create the bucket you specify in your output directory path for you. We went through this exercise so you understand that the root of your output directory on S3 will always be a bucket in some region. Note that you can use Amazon's \nweb interface\n to manipulate buckets, too.\n\n\nIf you've set up the AWS CLI properly, you can preprocess and align the example human dataset in the cloud with one command:\n\n\nrail-rna go elastic -m https://raw.githubusercontent.com/nellore/rail/master/ex/hg19_example.manifest -a hg19 -o s3://this-is-the-bucket-name-you-make-up/human_example --core-instance-type c3.2xlarge --master-instance-type c3.2xlarge -c 1\n\n\n\n\n. This command reserves two \nc3.2xlarge\n EC2 instances\n to execute a Rail-RNA job flow that dumps its output to \ns3://this-is-the-bucket-name-you-make-up/human_example\n. =There's always one master instance, whose type is specified with the \n--master-instance-type\n command-line parameter above; and the \n-c\n parameter specifies the number of core instances, whose type is specified with \n--core-instance-type\n. The master instance manages the Hadoop cluster that will run your job, scheduling and coordinating tasks that are executed on slave nodes---which for Rail-RNA are generally core instances. We recommend using at least 40 \nc3.2xlarge\n core instances for every hundred RNA-seq samples with 50 million reads each. You might also consider using \nc3.8xlarge\n instances, each of which has four times the resources of a \nc3.2xlarge\n instance; so in this case, the recommended ratio is at least 10 \nc3.8xlarge\n instances for every hundred RNA-seq samples with 50 million reads each. For our small example, we use only one core instance. The \n-a hg19\n parameter specifies that the input data should be aligned to hg19.\n\n\nWarning\n: Your job flow will execute in the default region you chose when you set up the AWS CLI unless you tack a \n--region \ndesired region\n onto the command above. If your output bucket is in a region different from your Elastic MapReduce cluster, transferring data between the cluster and S3 will take longer and slow down your job. \n\n\nExecuting the \nrail-rna go elastic ...\n command above gives the following output.\n\n\ntestcomputer:~ testuser$ rail-rna go elastic -m https://raw.githubusercontent.com/nellore/rail/master/ex/hg19_example.manifest -a hg19 -o s3://this-is-the-bucket-name-you-make-up/human_example --core-instance-type c3.2xlarge --master-instance-type c3.2xlarge -c 1\nLoading...\nChecked all files listed in manifest file.\nCopied Rail-RNA and bootstraps to S3.\n\n\u2200 Rail-RNA v0.2.0\nStarted job flow submission script on Saturday, Jul 18, 2015 at 08:50:34 PM EDT.\n\n~.oOo.\n\n\n00h:00m:00s |___| Read job flow from input JSON.\n00h:00m:10s |___| Verified that output directories on S3 are writable.\n00h:00m:23s |___| Set up output directories on S3.\n00h:00m:25s |___| Submitted job flow.\n*****Job flow ID is j-2NSJ3SJPGKC58 .*****\n*****Submission can be monitored at https://console.aws.amazon.com/elasticmapreduce/?region=eu-west-1#cluster-details:j-2NSJ3SJPGKC58 .*****\n00h:00m:25s |___| Opening URL in default browser, if possible.\n\n\n.oOo.~\n\nFinished job flow submission script on Saturday, Jul 18, 2015 at 08:51:00 PM EDT. Run time was 25.789 seconds.\n\n\n\n\nIf all goes well, your browser will open the Elastic MapReduce interface for monitoring your job flow. If that doesn't happen, the URL for viewing your job flow is included in Rail-RNA's output, and you can copy and paste it into your browser's address bar.\n\n\nExplore. Click on things. You'll first see something like\n\n\n\n\n. If you click on Steps, you'll see\n\n\n\n\n. These are the steps Rail-RNA will march through to align the human example. If you click on Bootstrap Actions, you'll see\n\n\n\n\n. When an Elastic MapReduce cluster starts up, its nodes are not equipped with the software Rail needs to run a job flow. Bootstrap actions are executed before the job flow begins to install this required software. Once the job flow is finished, all the software and any temporary files that have accumulated on the cluster are wiped. This is a virtue of using cloud computing to do your bioinformatics: you start with \nexactly\n the same machines and software configuration, making your results highly reproducible.\n\n\nIt's possible you'll want to click on Terminate soon after starting the job flow to avoid incurring any charges---and that's fine, but you may also want to delete Rail-RNA's detritus on S3 using the \nconsole\n. This includes the directories \ns3://this-is-the-bucket-name-you-make-up/human_example.dependencies\n and \ns3://this-is-the-bucket-name-you-make-up/human_example.logs\n. When Rail-RNA launches a job on Elastic MapReduce, it copies itself to S3 so the version of Rail you use in the cloud is precisely the version you use on your computer. This copy is stored in the directory that ends with \ndependencies\n. The directory that ends with \nlogs\n is used by Elastic MapReduce to record stats on your job flow. This is what you view in the Elastic MapReduce web interface. Another directory---\ns3://this-is-the-bucket-name-you-make-up/human_example.intermediate\n---will appear on S3 if you run the job flow from end to end. This directory stores intermediate data from one step that's streamed into some other step of the Rail-RNA pipeline. It shouldn't be touched until a job flow is complete, but afterwards, feel free to axe it. Both the \nintermediate\n and \ndependencies\n directories are purged automatically after four days to avoid your incurring extra S3 charges without your noticing. You can toggle how many days these directories remain on S3 with the \n--intermediate-lifetime\n command-line parameter.\n\n\nHere's a screenshot of the job flow in progress.\n\n\n\n\nIf you decided to continue the job flow, and your steps don't look something like this, something's gone wrong. Can't figure out what? Complain in the \nGitter\n.\n\n\nWhen the job flow is complete---\n\n\n\n\n---you can browse the outputs in the \nconsole\n. You'll find them at \ns3://this-is-the-bucket-name-you-make-up/human_example\n. The AWS CLI can also be used to list the contents of a directory like so:\n\n\ntestcomputer:~ testuser$ aws s3 ls s3://this-is-the-bucket-name-you-make-up/human_example/\n                           PRE alignments/\n                           PRE coverage_bigwigs/\n                           PRE cross_sample_results/\n                           PRE junctions_and_indels/\n\n\n\n\nYou can download all the results to your computer like so:\n\n\nmkdir /home/testuser/human_example\ncd /home/testuser/human_example\naws s3 cp s3://this-is-the-bucket-name-you-make-up/human_example/ ./ --recursive\n\n\n\n\n. \nWarning: transfers from S3 to non-EC2 computers \ncost money\n.\n\n\nTo divide the human example up into preprocess and align job flows, try the following commands.\n\n\nrail-rna prep elastic -m https://raw.githubusercontent.com/nellore/rail/master/ex/hg19_example.manifest -o s3://this-is-the-bucket-name-you-make-up/human_example --core-instance-type c3.2xlarge --master-instance-type c3.2xlarge -c 1\n\nrail-rna align elastic -m https://raw.githubusercontent.com/nellore/rail/master/ex/hg19_example.manifest -a hg19 -o s3://this-is-the-bucket-name-you-make-up/human_example --core-instance-type c3.2xlarge --master-instance-type c3.2xlarge -c 1\n\n\n\n\nYou should now know enough figure out how to use Rail-RNA to align your RNA-seq data! Refer to the \nDeliverables\n and \nReference\n for further details on, respectively, changing output formats and command-line parameters. To set up Elastic MapReduce for analyzing dbGaP-protected RNA-seq data with Rail-RNA, refer to \ndbGaP on EMR\n. Note that the instructions there may require that you contact your AWS administrator.", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorial/#trial-and-error", 
            "text": "We made a serious attempt to design Rail-RNA for people who don't read manuals to discourage improper use of the software. In general, you can get started by entering  rail-rna  in the shell. You'll obtain the following output, perhaps with a different version number.  error: too few arguments\nusage: rail-rna  job flow   mode   [args] \n\n   job flow        {prep, align, go}\n                     prep: preprocess reads listed in a required manifest\n                       file (specified with --manifest)\n                     align: align preprocessed reads (specified with --input)\n                     go: perform prep and align in succession\n   mode            {local, parallel, elastic}\n                     local: run Rail-RNA on this computer\n                     parallel: run Rail-RNA on all active IPython engines\n                     elastic: run Rail-RNA on Amazon Elastic MapReduce.\n                       Requires that the user sign up for Amazon Web Services\n\n\u2200 Rail-RNA v0.2.0 by Abhi Nellore (anellore@jhu.edu; nellore.github.io)\n\nRail-RNA is a scalable MapReduce pipeline that can analyze many RNA-seq\ndatasets at once. To view help for a given combination of  job flow  and mode , specify both, then add -h/--help.  It's quite possible to figure out how a substantial fraction Rail-RNA works by trial and error. Follow the prompts and fix your errors, which Rail tries to explain in detail when they're encountered. Please let us know in the  Gitter  if you're confused by any errors output. If you prefer to be guided through some examples, read on.", 
            "title": "Trial and error"
        }, 
        {
            "location": "/tutorial/#modes-and-job-flows", 
            "text": "Rail-RNA has three modes---    local , which runs the software on a single machine on up to as many processing cores as available. By default, Rail-RNA runs on the number of processing cores it detects on a machine less one to avoid monopolizing a single machine's resources. To adjust the number of cores on which Rail is run, use the  -p/--num-processes  command-line parameter described under Reference.    elastic , which runs the software on an Amazon Elastic MapReduce computer cluster in the cloud. This costs money: you rent computing capacity (on  Elastic Compute Cloud , or EC2) and storage (on  Simple Storage Service , or S3) from Amazon for the duration of your job flow and for as long as you keep your outputs in the cloud. Price information is  here . For a medium-size job (up to ~100 RNA-seq samples with ~50 million reads each), we tend to use a cluster of 40 c3.2xlarge machines, which costs $21/hour on demand but perhaps less than half that number on the spot market in the US Standard zone ( us-east-1 ). Learn more about the spot market and bidding on compute capacity  here .    parallel , which runs on an IPython Parallel cluster whose profile is specified at the command line. You must set up the IPython Parallel cluster yourself.    ---and three job flows---    prep , which if necessary downloads input FASTQ/FASTA files from a remote server and casts it in a form amenable for further analysis. This job flow must always be run before the  align  job flow.    align , which aligns preprocessed data and writes outputs explained in  Deliverables . Use the  --deliverables  option described there to control which of Rail-RNA's terminal outputs should be written.    go , which runs the  prep  and  align  job flows in succession.    To run some combination of mode and job flow, start your command with  rail-rna  mode   job flow   . Get help by entering  rail-rna  mode   job flow  -h  The reason for separating the  prep  and  align  job flows is that you may prefer to run these flows on different numbers of processing cores: if downloads are throttled by the remote server, and only a small number of threads can download input files concurrently, then you can limit the size of the computer cluster in  elastic  mode (with the  -c/--core-instance-count  parameter described in Reference) or the number of concurrently running threads in  local  mode (with the  -p/--num-processes  parameter described in Reference).  Align some RNA-seq samples with us in  local ,  parallel , and  elastic  modes below. We assume your login name is  testuser , and your home directory is  /home/testuser .  local  and  parallel  modes: Drosophila examples  Rail-RNA wraps both  Bowtie 1  and  Bowtie 2 , so you will need  both  Bowtie 1 and Bowtie 2 indexes of a genome assembly to perform alignment. It's easiest to download one of the  Illumina iGenomes , collections of reference sequences and indexes for popularly studied organisms. Here are explicit instructions:  Download the Drosophila melanogaster build dm3 iGenome, available  here . This is a 783-MB file. We assume you downloaded the dm3 iGenome to the directory  /home/testuser/Downloads . Unpack it by entering  cd /home/testuser/Downloads\ntar xvzf Drosophila_melanogaster_UCSC_dm3.tar.gz  in the shell. This could take a while, but once it's done, the Bowtie 1 index basename should be  /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome , while the Bowtie 2 index basename should be  /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome .  Warning:  You may think you already have Bowtie 1 and 2 indexes for a given genome assembly---perhaps dm3---but the contigs in one may not be exactly the same as the contigs in the other. When working with any reference in Rail-RNA, it is important to ensure that the Bowtie 1 and 2 indexes are built from the same FASTA. Check the  Bowtie 1 documentation  and  Bowtie 2 documentation  for information on how to build Bowtie indexes from reference FASTAs.  Now open your browser, and navigate to  this URL . You'll see the following text.  http://verve.webfactional.com/dm3_example_1_left.fastq       0       http://verve.webfactional.com/dm3_example_1_right.fastq        0       dm3_example-1-1\nhttp://verve.webfactional.com/dm3_example_2_left.fastq       0       http://verve.webfactional.com/dm3_example_2_right.fastq        0       dm3_example-2-1  This is a Rail-RNA manifest file, and it is required for all Rail-RNA runs. Its format mirrors the manifest file format of  Myrna , a predecessor of Rail. Each line corresponds to a different RNA-seq sample. A line for a single-end sample looks like this---  FASTQ/FASTA URL (tab) URL MD5 checksum or 0 (tab) sample label   ---while a line for a paired-end sample looks like this---  FASTQ/FASTA URL 1 (tab) URL 1 MD5 checksum or 0 (tab) FASTQ/FASTA URL 2 (tab) URL 2 MD5 checksum or 0 (tab) sample label   . URLs can be on the local filesystem, on the web, or if the  AWS CLI  is installed, on Amazon S3. If an input file is not on the local filesystem, it will be downloaded by Rail. Manifest files may be hosted remotely as well.  Our Drosophila example is composed of two paired-end biological replicates, and the raw data is hosted at  http://verve.webfactional.com  . Let's create a new  railtests  directory in  testuser 's home directory for running all our examples.   mkdir -p /home/testuser/railtests\ncd /home/testuser/railtests  Preprocessing and aligning the test data in  local  mode takes a single command:  rail-rna go local -x /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome -m https://raw.githubusercontent.com/nellore/rail/master/ex/dm3_example.manifest  . The run should complete after a few minutes. You can browse the output by entering  cd /home/testuser/railtests/rail-rna_out\nls  You'll find four directories:  alignments    cross_sample_results\ncoverage_bigwigs  junctions_and_indels  See  Deliverables  for information on how to interpret what's in these directories.  Commonly used options in  local  mode are   -p/--num-processes , which controls the number of processes Rail-RNA runs simultaneously. By default, Rail uses as many processing cores as your computer has less one so it doesn't monopolize resources. You may want to run on all cylinders by setting this equal to the number of available processing cores.   -o/--output , which changes the output directory. By default, this is  rail-rna_out  in the current directory.  See the Reference section for a comprehensive description of command-line parameters.  We could have run Rail-RNA in two steps: by preprocessing first and subsequently aligning. You can try this yourself by running the following commands in sequence.  cd /home/testuser/railtests\nrail-rna prep local -m https://raw.githubusercontent.com/nellore/rail/master/ex/dm3_example.manifest -o dmel_prepped -p 1\nrail-rna align local -i dmel_prepped -m https://raw.githubusercontent.com/nellore/rail/master/ex/dm3_example.manifest -x home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome -f  The  rail-rna prep local  command above is executed on a single thread (with  -p 1 ) to ensure that only one download is performed at a time, which would be useful if our downloads were throttled. In the  rail-rna align local  command, the parameter  -f/--force  tells Rail to overwrite any existing outputs. You should end up with exactly the same output you obtained when you ran the  rail-rna go local  command.  Now you'll run an example that will break Rail. Navigate to  this URL , a manifest file that should look like this:  http://verve.webfactional.com/bad.fastq       0        bad-1-1  It's a single-end sample called \"bad,\" presumably because it's not good. Visit http://verve.webfactional.com/bad.fastq next. The file will be downloaded, or it will be displayed in your browser. If it's downloaded, navigate to the appropriate folder at the shell prompt, and enter  cat bad.fastq  . Either way, you'll see  @badrecord\nATACAGATGACAGATGACAGGGTAGAGACAAATAGACAGATGACGATGGACAGATGACAGATAGAACAGATAGAGA\n+\nIIIIIIIIIIIIII\n@goodrecord\nATGGCATCAGTCAAGTCAAGATTACTAGTAGCCATACAAGATACATCGTTTAACGATTGTGGCACATACGTCACCA\n+\nIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII  There's one sequence labeled \"badrecord\" and another labeled \"goodrecord\". \"badrecord\" is bad because its read sequence isn't the same length as its quality sequence. Without being told otherwise, Rail-RNA chokes on bad records, but it's good to see how it chokes to learn how to diagnose problems. Run Rail-RNA on the bad manifest file like so---  cd /home/testuser/railtests\nrail-rna go local -x /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome -m https://raw.githubusercontent.com/nellore/rail/master/ex/bad.manifest -o ./bad  ---and you'll ultimately obtain output like this:  \u2200 Rail-RNA v0.2.0\nStarted job flow on Saturday, Jul 18, 2015 at 07:36:29 PM EDT.\n\n~.oOo. \n\n00h:00m:00s |___| Step 1/24: Count lines in input files\n00h:00m:01s |___|     Completed 1 task.\n00h:00m:01s |___|     Deleted temporary files.\n00h:00m:01s |___| Step 2/24: Assign reads to preprocessing tasks\n00h:00m:01s |___|     Partitioned 1 input into tasks.\n00h:00m:03s |___|     Completed 1 task.\n00h:00m:03s |___|     Deleted temporary files.\n00h:00m:03s |___| Step 3/24: Preprocess reads\n*****Errors encountered*****\nStreaming command  cat /var/folders/rs/zytvc1753p9_bth4lrhbm7c80000gn/T/tmpCxbBKB/0 | /usr/local/raildotbio/pypy-2.5.0-osx64/bin/pypy /usr/local/raildotbio/rail-rna/rna/steps/preprocess.py --nucs-per-file=100000000 --gzip-output --push=/home/testuser/railtests/rail-rna_logs/preprocess/push --gzip-level 3   --bin-qualities  /home/testuser/railtests/rail-rna_logs/preprocess/0 2 /home/testuser/railtests/rail-rna_logs/preprocess/dp.map.log/0.0.log  failed; exit level was 1.\nJob flow failed on Saturday, Jul 18, 2015 at 07:36:34 PM EDT. Run time was 5.503 seconds.\nTo start this job flow from where it left off, run:\n/usr/local/raildotbio/pypy-2.5.0-osx64/bin/pypy /usr/local/raildotbio/rail-rna/dooplicity/emr_simulator.py -j /home/testuser/railtests/rail-rna_logs/resume_flow_WDEZ24GVC0TM.json -b /usr/local/raildotbio/rail-rna/rna/driver/rail-rna.txt -l /home/testuser/railtests/rail-rna_logs/flow.2015-07-18T19:36:27.934937.log -f --max-attempts 1 --num-processes 3\nTraceback (most recent call last):\n  File  app_main.py , line 75, in run_toplevel\n  File  /usr/local/raildotbio/rail-rna/dooplicity/emr_simulator.py , line 1963, in  module \n    args.scratch, args.common, args.sort, args.max_attempts)\n  File  /usr/local/raildotbio/rail-rna/dooplicity/emr_simulator.py , line 1719, in run_simulation\n    max_attempts=max_attempts\n  File  /usr/local/raildotbio/rail-rna/dooplicity/emr_simulator.py , line 1290, in execute_balanced_job_with_retries\n    raise RuntimeError\nRuntimeError  Rail is telling us that something went wrong with a command during preprocessing. Note the following.    Rail gives you a command you can use to resume a job flow if it's failed for a fixable reason. One possible reason is that you ran out of space on disk during a job flow, and you need to delete some of your files. You can highlight and copy the command that resumes your job flow with  Command+C  on a Mac or  CTRL+C  in Linux. If somehow you lose the resume command, you'll also find it in the last written file whose extension is  .log  in the log directory  /home/testuser/railtests/rail-rna_logs . Enter   ls -tr /home/testuser/railtests/rail-rna_logs/*.log | tail -n 1  to get its path.    There's a log file in the command that failed, right after the  2 . This file will tell you why the job flow failed. Open it with  less :  less /home/testuser/dmel/rail-rna_logs/preprocess/dp.map.log/0.0.log  . (You should  less  whatever log file appears for you after the  2 ). At the bottom of the file is the exception that made the job flow fail:  Created local destination directory \"/var/folders/rs/zytvc1753p9_bth4lrhbm7c80000gn/T/tmpQJHduB\".\nRetrieving URL \"http://verve.webfactional.com/bad.fastq\"...\nRange of quality values found from random sample of 10000 records is (73, 73).\nGuessed Phred64 encoding.\nTraceback (most recent call last):\n  File \"app_main.py\", line 75, in run_toplevel\n  File \"/usr/local/raildotbio/rail-rna/rna/steps/preprocess.py\", line 863, in  module \n    mover=mover)\n  File \"/usr/local/raildotbio/rail-rna/rna/steps/preprocess.py\", line 557, in go\n    ) % (line_numbers[i], sources[i])\nAssertionError: Length of read sequence does not match length of quality string at line 4 of file \"/var/folders/rs/zytvc1753p9_bth4lrhbm7c80000gn/T/tmpQJHduB/bad.fastq\".  . So Rail noticed  @badrecord  was a bad record and failed. You can have Rail ignore bad records during preprocessing using the  --ignore-bad-records  command-line parameter:  rail-rna go local -x /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome -m https://raw.githubusercontent.com/nellore/rail/master/ex/bad.manifest -o ./bad -f --ignore-bad-records  . This job flow should succeed, and you'll end up with exactly one aligned read in the directory  /home/testuser/railtests/bad/alignments . Ignoring bad records may be useful when you're analyzing hundreds of samples and, for example, one file is truncated, but you'd like to analyze all its available reads anyway. Sometimes, you can't anticipate the integrity of the data you're analyzing, but you don't want your job flow to fail because of a handful of bad records.    To test Rail-RNA in  parallel  mode, you should have IPython installed. If the Rail-RNA installer didn't prompt you to install it, you already have it. Detailed instructions on starting IPython clusters over many different cluster configurations may be found  here . The IPython documentation around this link teaches you how to create a profile for a cluster configuration and how to start a cluster. An IPython cluster is nothing but a collection of Python interpreters (\"engines\") running on processing cores distributed across some networked computers. Rail-RNA does not start an IPython cluster for you; rather, it detects a running IPython cluster and runs itself over that. You can choose the IPython profile Rail-RNA should use with the  --ipython-profile  command-line parameter. If left unspecified, this is taken to be the default profile.  For testing purposes, you can run an IPython cluster on just your machine by entering  ipcluster start -n  k   , where  k  is the number of IPython engine processes you want to run.  k  is analogous to the  -p/--num-processes  parameter of Rail-RNA in local mode. Now rerun the first Drosophila example except in  parallel  mode by entering  rail-rna go parallel -x /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/BowtieIndex/genome /home/testuser/Downloads/Drosophila_melanogaster/UCSC/dm3/Sequence/Bowtie2Index/genome -m https://raw.githubusercontent.com/nellore/rail/master/ex/dm3_example.manifest -f  What happens is pretty unimpressive: Rail runs like it does in local mode. But IPython Parallel lends Rail versatility: there are many different IPython cluster configurations you can set up on, say, your institutional computer cluster. But there is one caveat: in  parallel  mode, you  must  make sure the output directory (specified with  -o/--output ) and the log directory (specified with  --log ) are at paths accessible to  all  the nodes in your cluster running IPython engines. Rail-RNA aggregates intermediate data in the log directory between any two successive steps, and all nodes must be able to stream data from that directory to complete their assigned tasks. Moreover, outputs must go to the same place, and some paths on a cluster refer to different places on different nodes. For example, you probably shouldn't make a subdirectory of  /tmp  your output or log directory on a conventional computer cluster because  /tmp  tends to be a node-local temporary directory.  elastic  mode: a human example  Warning: running this example will cost money , but probably no more than US$5. Read our  disclaimer  before using  elastic  mode.  You should have performed  these  steps to set up the AWS CLI and Elastic MapReduce before attempting the example.  Check out  this manifest file  in your browser. Listed are two single-end human samples with just 20,000 reads each. They were generated with expression profiles of two  GEUVADIS  lymphoblastoid cell line samples in a way we describe in our  paper , but because there are so few reads, you probably couldn't tell which GEUVADIS samples we used if you didn't have the sample labels in the manifest file.  Let's use Elastic MapReduce to both preprocess and align these data. Since you'll be storing the results on Amazon's  Simple Storage Service  (S3), you should first read up on  working with buckets . The short story is that on S3, a bucket is something like a directory in a filesystem, except   Unless you specify otherwise, only your account can access it.  Bucket names are globally unique: if someone else has taken a bucket name in any Amazon region, you can't have it.  Underscores in bucket names are a bad idea when you use Elastic MapReduce. They cause weird problems. Don't ask why; just go with it.  A bucket is located in a specific Amazon region---that is, a bucket has a physical location in an Amazon data center, and that's where your files go.   You can create a bucket at the command line using the AWS CLI. Enter  aws s3 mb s3://this-is-the-bucket-name-you-make-up --region  a valid region   . A list of valid regions is available  here . Since you specified your default region when configuring the AWS CLI, if you leave the  --region  part out above, the bucket will be created in your default region---typically  us-east-1 .  But you don't  have  to create a bucket yourself before using Rail; if it doesn't already exist, Rail will automatically create the bucket you specify in your output directory path for you. We went through this exercise so you understand that the root of your output directory on S3 will always be a bucket in some region. Note that you can use Amazon's  web interface  to manipulate buckets, too.  If you've set up the AWS CLI properly, you can preprocess and align the example human dataset in the cloud with one command:  rail-rna go elastic -m https://raw.githubusercontent.com/nellore/rail/master/ex/hg19_example.manifest -a hg19 -o s3://this-is-the-bucket-name-you-make-up/human_example --core-instance-type c3.2xlarge --master-instance-type c3.2xlarge -c 1  . This command reserves two  c3.2xlarge  EC2 instances  to execute a Rail-RNA job flow that dumps its output to  s3://this-is-the-bucket-name-you-make-up/human_example . =There's always one master instance, whose type is specified with the  --master-instance-type  command-line parameter above; and the  -c  parameter specifies the number of core instances, whose type is specified with  --core-instance-type . The master instance manages the Hadoop cluster that will run your job, scheduling and coordinating tasks that are executed on slave nodes---which for Rail-RNA are generally core instances. We recommend using at least 40  c3.2xlarge  core instances for every hundred RNA-seq samples with 50 million reads each. You might also consider using  c3.8xlarge  instances, each of which has four times the resources of a  c3.2xlarge  instance; so in this case, the recommended ratio is at least 10  c3.8xlarge  instances for every hundred RNA-seq samples with 50 million reads each. For our small example, we use only one core instance. The  -a hg19  parameter specifies that the input data should be aligned to hg19.  Warning : Your job flow will execute in the default region you chose when you set up the AWS CLI unless you tack a  --region  desired region  onto the command above. If your output bucket is in a region different from your Elastic MapReduce cluster, transferring data between the cluster and S3 will take longer and slow down your job.   Executing the  rail-rna go elastic ...  command above gives the following output.  testcomputer:~ testuser$ rail-rna go elastic -m https://raw.githubusercontent.com/nellore/rail/master/ex/hg19_example.manifest -a hg19 -o s3://this-is-the-bucket-name-you-make-up/human_example --core-instance-type c3.2xlarge --master-instance-type c3.2xlarge -c 1\nLoading...\nChecked all files listed in manifest file.\nCopied Rail-RNA and bootstraps to S3.\n\n\u2200 Rail-RNA v0.2.0\nStarted job flow submission script on Saturday, Jul 18, 2015 at 08:50:34 PM EDT.\n\n~.oOo. \n\n00h:00m:00s |___| Read job flow from input JSON.\n00h:00m:10s |___| Verified that output directories on S3 are writable.\n00h:00m:23s |___| Set up output directories on S3.\n00h:00m:25s |___| Submitted job flow.\n*****Job flow ID is j-2NSJ3SJPGKC58 .*****\n*****Submission can be monitored at https://console.aws.amazon.com/elasticmapreduce/?region=eu-west-1#cluster-details:j-2NSJ3SJPGKC58 .*****\n00h:00m:25s |___| Opening URL in default browser, if possible. .oOo.~\n\nFinished job flow submission script on Saturday, Jul 18, 2015 at 08:51:00 PM EDT. Run time was 25.789 seconds.  If all goes well, your browser will open the Elastic MapReduce interface for monitoring your job flow. If that doesn't happen, the URL for viewing your job flow is included in Rail-RNA's output, and you can copy and paste it into your browser's address bar.  Explore. Click on things. You'll first see something like   . If you click on Steps, you'll see   . These are the steps Rail-RNA will march through to align the human example. If you click on Bootstrap Actions, you'll see   . When an Elastic MapReduce cluster starts up, its nodes are not equipped with the software Rail needs to run a job flow. Bootstrap actions are executed before the job flow begins to install this required software. Once the job flow is finished, all the software and any temporary files that have accumulated on the cluster are wiped. This is a virtue of using cloud computing to do your bioinformatics: you start with  exactly  the same machines and software configuration, making your results highly reproducible.  It's possible you'll want to click on Terminate soon after starting the job flow to avoid incurring any charges---and that's fine, but you may also want to delete Rail-RNA's detritus on S3 using the  console . This includes the directories  s3://this-is-the-bucket-name-you-make-up/human_example.dependencies  and  s3://this-is-the-bucket-name-you-make-up/human_example.logs . When Rail-RNA launches a job on Elastic MapReduce, it copies itself to S3 so the version of Rail you use in the cloud is precisely the version you use on your computer. This copy is stored in the directory that ends with  dependencies . The directory that ends with  logs  is used by Elastic MapReduce to record stats on your job flow. This is what you view in the Elastic MapReduce web interface. Another directory--- s3://this-is-the-bucket-name-you-make-up/human_example.intermediate ---will appear on S3 if you run the job flow from end to end. This directory stores intermediate data from one step that's streamed into some other step of the Rail-RNA pipeline. It shouldn't be touched until a job flow is complete, but afterwards, feel free to axe it. Both the  intermediate  and  dependencies  directories are purged automatically after four days to avoid your incurring extra S3 charges without your noticing. You can toggle how many days these directories remain on S3 with the  --intermediate-lifetime  command-line parameter.  Here's a screenshot of the job flow in progress.   If you decided to continue the job flow, and your steps don't look something like this, something's gone wrong. Can't figure out what? Complain in the  Gitter .  When the job flow is complete---   ---you can browse the outputs in the  console . You'll find them at  s3://this-is-the-bucket-name-you-make-up/human_example . The AWS CLI can also be used to list the contents of a directory like so:  testcomputer:~ testuser$ aws s3 ls s3://this-is-the-bucket-name-you-make-up/human_example/\n                           PRE alignments/\n                           PRE coverage_bigwigs/\n                           PRE cross_sample_results/\n                           PRE junctions_and_indels/  You can download all the results to your computer like so:  mkdir /home/testuser/human_example\ncd /home/testuser/human_example\naws s3 cp s3://this-is-the-bucket-name-you-make-up/human_example/ ./ --recursive  .  Warning: transfers from S3 to non-EC2 computers  cost money .  To divide the human example up into preprocess and align job flows, try the following commands.  rail-rna prep elastic -m https://raw.githubusercontent.com/nellore/rail/master/ex/hg19_example.manifest -o s3://this-is-the-bucket-name-you-make-up/human_example --core-instance-type c3.2xlarge --master-instance-type c3.2xlarge -c 1\n\nrail-rna align elastic -m https://raw.githubusercontent.com/nellore/rail/master/ex/hg19_example.manifest -a hg19 -o s3://this-is-the-bucket-name-you-make-up/human_example --core-instance-type c3.2xlarge --master-instance-type c3.2xlarge -c 1  You should now know enough figure out how to use Rail-RNA to align your RNA-seq data! Refer to the  Deliverables  and  Reference  for further details on, respectively, changing output formats and command-line parameters. To set up Elastic MapReduce for analyzing dbGaP-protected RNA-seq data with Rail-RNA, refer to  dbGaP on EMR . Note that the instructions there may require that you contact your AWS administrator.", 
            "title": "Modes and job flows"
        }, 
        {
            "location": "/deliverables/", 
            "text": "Choosing deliverables\n\n\nThere are several classes of outputs. Each is described in a different section below. You can toggle which outputs are written with the \n-d/--deliverables\n command-line parameter for \ngo\n and \nalign\n job flows. By default \n--deliverables\n is set to \ntsv idx bw bed\n, which is all the outputs described below besides the alignment BAMs. (Do you really need them? Suppressing the output saves a lot of space--which is especially useful on S3, where storage is rented.) If you do need BAM output, invoke \n--deliverables tsv idx bw bed bam\n.\n\n\ntsv\n: cross-sample tables\n\n\nThese are gzip-compressed cross-sample outputs. They appear in the \ncross_sample_results\n subdirectory of the output directory.\n\n\n\n\ncounts.tsv.gz\n: A labeled matrix whose (i, j)th element takes the form \"x\nij\n,y\nij\n\", where x\nij\n is the number of primary alignments in sample i to group j, and y\nij\n is the number of uniquely aligning reads in group j. Here, a group is a chromosome, the set of unmapped reads U, the set of mapped reads M, or the set of all reads A; and \"uniquely aligning read\" means that there is exactly one alignment of the read with the highest alignment score. For group U, x\nij\n = y\nij\n. For group M, x\nij\n is the sum of all the x\nij\ns in the chromosome columns. For group A, x\nij\n is the total number of reads, while y\nij\n is the sum of the y\nij\ns in groups U and M.\n\n\n\n\njunctions.tsv.gz\n: a labeled matrix whose (i, j)th element is the number of reads in sample j covering intron i. Introns are in the first column. Each one takes the form\n\n\nchromosome\n;\nstrand (+/-)\n;\n1-based start position (inclusive)\n;\n1-based end position (exclusive)\n.\n\n\n\nSample names are in the first row.\n  * \n[insertion|deletion]s.tsv.gz\n: a labeled matrix whose (i, j)th element is the number of reads in sample j covering [insertion|deletion] i. [Insertion|deletion]s are in the first column. An insertion takes the form\n\n\nchromosome\n;\ninserted base sequence\n;\n1-based position of the last base before the insertion\n;\n1-based position of the last base before the insertion\n\n\n\n\nA deletion takes the form\n\n\nchromosome\n;\ndeleted base sequence\n;\n1-based position of the first deleted base\n;\n1-based position of the first base after the deletion\n\n\n\n\n\n\n\n\nnormalization.tsv.gz\n: each row takes the form \nsample name\n(tab)\n--normalize-percentile\n normalization factor for coverage vector composed of primary alignments\n(tab)\n--normalize-percentile\n normalization factor for coverage vector composed of unique alignments\n. The default percentile is \n0.75\n, corresponding to \nupper-quartile normalization\n.\n\n\n\n\n\n\nidx\n: isofrag index\n\n\nThis refers to the file \nisofrags.tar.gz\n that appears in the \ncross_sample_results\n subdirectory of the output directory. In general, it should be left gzipped; it can be used to align a new sample to the transcript fragments obtained from an old Rail-RNA run. Say you've aligned 500 samples from the same tissue type with Rail, and you think the list of exon-exon junctions you found is pretty comprehensive. Now say you just obtained another 10 samples from the same tissue type, and you want to align them. You can skip looking for novel junctions in those 10 samples by passing Rail-RNA's \ngo\n or \nalign\n job flow the path to this file as an argument of the \n--isofrag-idx\n command-line parameter.\n\n\nbw\n: coverage vectors\n\n\nThese are coverage bigWigs, and they appear in the \ncoverage_bigwigs\n subdirectory of the output directory. There are two for each sample. One is called \nsample name\n.bw\n, and it encodes coverage of the genome by primary alignments. The other is called \nsample name\n.unique.bw\n, and it encodes coverage of the genome by uniquely aligning reads, where a uniquely aligning read has exactly one alignment with the highest alignment score. There are also coverage bigwigs for each chromosome. \n[mean|median].\nchr\n.bw\n encodes the [mean|median] coverage at each base across samples on chromosome \nchr\n by primary alignments, and \n[mean|median].\nchr\n.bw\n encodes the [mean|median] coverage at each base on chromosome \nchr\n by uniquely aligning reads. To normalize each sample's contribution to the [mean|median], the number of reads covering a given base of the genome is multiplied by some library size parameter (by default 40 million) and divided by the number of mapped reads in the sample. Leo Collado-Torres describes how to use bigWigs as input to downstream analysis tools like \nderfinder\n, \nedgeR-robust\n, and \nDESeq2\n \nhere\n. Note that a bigWig is typically an order of magnitude smaller than a BAM.\n\n\nbam\n: alignments\n\n\nThese are alignment BAMs and their corresponding indexes (BAIs) that appear in the \nalignments\n subdirectory of the output directory. By default, there's one BAM per sample per chromosome taking the form \nalignments.\nsample name\n.\nchr\n.bam\n. You can use the \n--do-not-output-bam-by-chr\n command-line parameter for \ngo\n and \nalign\n job flows to output one BAM per sample. In that case, the filename takes the form \nalignments.\nsample name\n.bam\n. Note that outputting BAMs by chromosome is in general faster because it increases parallelism.\n\n\nbed\n: junctions and indels\n\n\nThese are BED files that mirror TopHat 2's output BEDs. They appear in the \njunctions_and_indels\n subdirectory of the output directory. For each sample \nsample name\n, there are three BEDs: \njunctions.\nsample name\n.bed\n, \ninsertions.\nsample name\n.bed\n, and \ndeletions.\nsample name\n.bed\n. Quotes in the following statements are from the \nTopHat 2 manual\n. Recall that BED always uses 0-based coordinates.\n  * \njunctions.\nsample name\n.bed\n: \"Each junction consists of two connected BED blocks, where each block is as long as the maximal overhang of any read spanning the junction. The score is the number of alignments spanning the junction.\"\n  * \ndeletions.\nsample name\n.bed\n: \"chromLeft refers to the first genomic base of the deletion.\"\n  * \ninsertions.\nsample name\n.bed\n: \"chromLeft refers to the last genomic base before the insertion.\"", 
            "title": "Deliverables"
        }, 
        {
            "location": "/deliverables/#choosing-deliverables", 
            "text": "There are several classes of outputs. Each is described in a different section below. You can toggle which outputs are written with the  -d/--deliverables  command-line parameter for  go  and  align  job flows. By default  --deliverables  is set to  tsv idx bw bed , which is all the outputs described below besides the alignment BAMs. (Do you really need them? Suppressing the output saves a lot of space--which is especially useful on S3, where storage is rented.) If you do need BAM output, invoke  --deliverables tsv idx bw bed bam .  tsv : cross-sample tables  These are gzip-compressed cross-sample outputs. They appear in the  cross_sample_results  subdirectory of the output directory.   counts.tsv.gz : A labeled matrix whose (i, j)th element takes the form \"x ij ,y ij \", where x ij  is the number of primary alignments in sample i to group j, and y ij  is the number of uniquely aligning reads in group j. Here, a group is a chromosome, the set of unmapped reads U, the set of mapped reads M, or the set of all reads A; and \"uniquely aligning read\" means that there is exactly one alignment of the read with the highest alignment score. For group U, x ij  = y ij . For group M, x ij  is the sum of all the x ij s in the chromosome columns. For group A, x ij  is the total number of reads, while y ij  is the sum of the y ij s in groups U and M.   junctions.tsv.gz : a labeled matrix whose (i, j)th element is the number of reads in sample j covering intron i. Introns are in the first column. Each one takes the form  chromosome ; strand (+/-) ; 1-based start position (inclusive) ; 1-based end position (exclusive) .  Sample names are in the first row.\n  *  [insertion|deletion]s.tsv.gz : a labeled matrix whose (i, j)th element is the number of reads in sample j covering [insertion|deletion] i. [Insertion|deletion]s are in the first column. An insertion takes the form  chromosome ; inserted base sequence ; 1-based position of the last base before the insertion ; 1-based position of the last base before the insertion   A deletion takes the form  chromosome ; deleted base sequence ; 1-based position of the first deleted base ; 1-based position of the first base after the deletion     normalization.tsv.gz : each row takes the form  sample name (tab) --normalize-percentile  normalization factor for coverage vector composed of primary alignments (tab) --normalize-percentile  normalization factor for coverage vector composed of unique alignments . The default percentile is  0.75 , corresponding to  upper-quartile normalization .    idx : isofrag index  This refers to the file  isofrags.tar.gz  that appears in the  cross_sample_results  subdirectory of the output directory. In general, it should be left gzipped; it can be used to align a new sample to the transcript fragments obtained from an old Rail-RNA run. Say you've aligned 500 samples from the same tissue type with Rail, and you think the list of exon-exon junctions you found is pretty comprehensive. Now say you just obtained another 10 samples from the same tissue type, and you want to align them. You can skip looking for novel junctions in those 10 samples by passing Rail-RNA's  go  or  align  job flow the path to this file as an argument of the  --isofrag-idx  command-line parameter.  bw : coverage vectors  These are coverage bigWigs, and they appear in the  coverage_bigwigs  subdirectory of the output directory. There are two for each sample. One is called  sample name .bw , and it encodes coverage of the genome by primary alignments. The other is called  sample name .unique.bw , and it encodes coverage of the genome by uniquely aligning reads, where a uniquely aligning read has exactly one alignment with the highest alignment score. There are also coverage bigwigs for each chromosome.  [mean|median]. chr .bw  encodes the [mean|median] coverage at each base across samples on chromosome  chr  by primary alignments, and  [mean|median]. chr .bw  encodes the [mean|median] coverage at each base on chromosome  chr  by uniquely aligning reads. To normalize each sample's contribution to the [mean|median], the number of reads covering a given base of the genome is multiplied by some library size parameter (by default 40 million) and divided by the number of mapped reads in the sample. Leo Collado-Torres describes how to use bigWigs as input to downstream analysis tools like  derfinder ,  edgeR-robust , and  DESeq2   here . Note that a bigWig is typically an order of magnitude smaller than a BAM.  bam : alignments  These are alignment BAMs and their corresponding indexes (BAIs) that appear in the  alignments  subdirectory of the output directory. By default, there's one BAM per sample per chromosome taking the form  alignments. sample name . chr .bam . You can use the  --do-not-output-bam-by-chr  command-line parameter for  go  and  align  job flows to output one BAM per sample. In that case, the filename takes the form  alignments. sample name .bam . Note that outputting BAMs by chromosome is in general faster because it increases parallelism.  bed : junctions and indels  These are BED files that mirror TopHat 2's output BEDs. They appear in the  junctions_and_indels  subdirectory of the output directory. For each sample  sample name , there are three BEDs:  junctions. sample name .bed ,  insertions. sample name .bed , and  deletions. sample name .bed . Quotes in the following statements are from the  TopHat 2 manual . Recall that BED always uses 0-based coordinates.\n  *  junctions. sample name .bed : \"Each junction consists of two connected BED blocks, where each block is as long as the maximal overhang of any read spanning the junction. The score is the number of alignments spanning the junction.\"\n  *  deletions. sample name .bed : \"chromLeft refers to the first genomic base of the deletion.\"\n  *  insertions. sample name .bed : \"chromLeft refers to the last genomic base before the insertion.\"", 
            "title": "Choosing deliverables"
        }, 
        {
            "location": "/dbgap/", 
            "text": "Securely analyze dbGaP-protected data in the cloud\n\n\nThe \nNational Institutes of Health (NIH)\n maintains security \nrequirements\n and \nrecommmendations\n for analyzing controlled-access genomic data, including \ndbGaP\n-protected data. With some setup beyond installation, Rail-RNA can analyze dbGaP-protected RNA-seq data on Amazon Elastic MapReduce in a way that complies with these policies. \nAmazon Web Services\n (AWS) has also released complementary documents on securing its resources, including some \nsecurity best practices\n and the whitepaper \nArchitecting for Genomic Data Security and Compliance in the Cloud\n.\n\n\nRail-RNA ensures encryption of all data it handles at rest---on the Elastic MapReduce cluster and on S3---as well as in transit. Rail-RNA also ensures that the Elastic MapReduce cluster is sufficiently isolated from the internet by launching all job flows into a \nVirtual Private Cloud\n (VPC) augmented by several security features. (See \nthis section\n for more information.) The steps below create a new \nAWS IAM\n account especially for analyzing dbGaP-protected data. To perform these steps, both user and AWS site administrator should be available. (For many investigators, user and administrator will be the same person.) It is recommended that they are physically together to minimize passing of credentials. The user should already have Rail-RNA \ninstalled\n on their local machine, including the \nAWS Command Line Interface (CLI)\n. The user should also have \nrequested access\n to some dbGaP-protected sample on the \nSequence Read Archive\n (SRA) and received a key file with an \nngc\n extension.\n\n\nAnalysis of data on \nCGHub\n is currently unsupported, but we hope to add support in the near future.\n\n\nSet up an administrator account (administrator)\n\n\nThese steps should be performed if the site administrator is new to AWS.\n\n\n\n\nNavigate to \nhttp://aws.amazon.com/free\n in your web browser.\n\n\nClick \nCreate a free account\n.\n\n\nCheck the \nI am a new user\n box and and continue to follow the instructions to create your new account. You'll enter, for example, contact and payment information. Note that the \nBasic\n level of service is sufficient for using Rail-RNA.\n\n\nMake a note of your account number.\n\n\nLog into the [AWS console] using the new account's email address and password.\n\n\nClick on the arrow next to your user name in the gray banner at the top of the page.\n\n\nSelect \nMy Account\n, and the \nAccount Id\n will be displayed at the top of the page.\n\n\n\n\n\n\nSecure the account\n\n\nLog into the \nAWS console\n using the new account's email address and password.\n\n\nOpen the \nIdentity and Access Management\n page.\n\n\n\nUnder \nSecurity Status\n, click \nActivate MFA on your root account\n, then click \nManage MFA\n, and follow the instructions to enable multi-factor authentication. We use a virtual MFA device (smartphone) with Google Authenticator.\n \n\n\nUnder \nApply an IAM password policy\n, click \nManage Password Policy\n.\n\n\nConfigure the password policy according to the requirements mentioned in the \nNIH Security Best Practices for Controlled-Access Data Subject to the NIH Genomic Data Sharing (GDS) Policy\n. This usually entails the following, but please note that your institution may impose more stringent requirements:\n\n\nRequiring a minimum password length of 12\n\n\nRequiring at least one uppercase letter\n\n\nRequiring at least one lowercase letter\n\n\nRequiring at least one number\n\n\nRequiring at least one non-alphanumeric character\n\n\nEnable password expiration after 120 days\n\n\n\n\n\n\n\nClick \nApply password policy\n.\n\n\n\n\n\n\n\n\nSet up a new IAM user (administrator \n user)\n\n\nDuring this process, it is best for the account administrator to sit with the user to minimize passing credentials. \n\n\n\n\n\n\nAdministrator:\n create new IAM user.\n\n\n\n\nFrom the new user's computer, log into the \nAWS Console\n and select \nIdentity and Access Management\n.\n\n\nClick \nUsers\n on the left pane, then \nCreate New Users\n on the right pane.\n\n\n\nEnter the new user's username. We call the new user \ndbgapuser\n in the screenshot. Check the \nGenerate an access key for each user\n checkbox, and click \nCreate\n.\n\n\n\nClick \nDownload Credentials\n. These credentials (\ncredentials.csv\n) include the AWS Access Key ID and AWS Secret Access Key. It is recommended that the file containing the credentials be made readable only by the user immediately. The credentials should never be shared, intentionally or inadvertently, with anyone else.\n\n\n\n\n\n\n\n\nUser:\n register credentials with the AWS CLI by entering\n\n\naws configure --profile dbgap\n\n\n\nat a terminal prompt on the user's computer. Enter the AWS Access Key ID, AWS Secret Access Key, and a default region as prompted. We recommend using the \nus-east-1\n because its connection to dbGaP-protected data on SRA appears to be fastest. A default output format need not be specified. Now the new user can issue AWS API calls via the AWS CLI. \nIt is recommended that credentials file that was just downloaded is now deleted.\n\n\n\n\n\n\nAdministrator:\n Set user's password.\n\n\n\n\nReturn to the \nAWS Console\n, again click \nIdentity and Access Management\n, again click \nUsers\n on the left sidebar, and select the new user. Under \nUser Actions\n, click \nManage Password\n.\n\n\n\nSelect \nAssign an auto-generated password\n, check the \nRequire user to create a new password at next sign-in\n box, and click \nApply\n.\n\n\n\nClick \nDownload Credentials\n. The new credentials file \ncredentials (2).csv\n contains the username, the auto-generated password, and the URL for the account-specific login page.\n\n\n\n\n\n\n\n\nUser:\n navigate to the login page URL from \ncredentials (2).csv\n, log in, and change the password as prompted.\n\n\n\n\n\n\n\n\n\nCreate a secure CloudFormation stack (administrator)\n\n\nCloudFormation\n facilitates creation and management of a group of related AWS resources. Rail-RNA is bundled with a CloudFormation template for creating a \nVirtual Private Cloud\n (VPC) with a \nsingle public subnet\n. A Rail-RNA job flow that analyzes dbGaP data is launched into this subnet. The VPC is supplemented by several security features, including\n\n\n\n\na VPC endpoint for S3, which ensures that the connection between the Elastic MapReduce cluster and S3 is private.\n\n\nsecurity groups that block all inbound traffic to the cluster from the internet except from the Elastic MapReduce webservice.\n\n\nthe creation of a secure bucket on S3 into which Rail-RNA should write all its output when operating on dbGaP-protected data. The bucket has an attached policy barring uploads that do not have server-side encryption (AES256) turned on.\n\n\nCloudTrail\n logs recording AWS API calls. These are written to the secure bucket.\n\n\n\n\nThe user will find the CloudFormation template at \n$RAILDOTBIO/cloudformation/dbgap.template\n and can send it to the administrator, but it is recommended that the administrator grab the latest version of the template \nhere\n. Implement it by following these steps. (If the administrator already has CloudTrail turned on, they may not work, causing a rollback. An administrator satisfied with their CloudTrail configuration may instead want to use \nthis alternative CloudFormation template\n, which creates the VPC but does not attempt to set up CloudTrail.)\n\n\n\n\nClick \nCloudFormation\n in the AWS console, making sure the region in the upper-right corner of the screen is the same as the user's default region (typically \nus-east-1\n, i.e., N. Virginia).\n\n\n\nClick \nCreate Stack\n.\n\n\n\nUnder \nChoose a template\n, opt to upload \ndbgap.template\n to Amazon S3, and click \nNext\n.\n\n\n\nOn the next screen:\n\n\nNext to \nStack name\n, write \"dbgap\".\n\n\nNext to \nParameters\n, let the user type the name of a secure bucket into which they will write all of Rail-RNA's output. The bucket name should not have been taken by any other S3 user.\n\n\n\n\n\n\n\nClick \nNext\n and \nNext\n again, then click \nCreate\n and wait for the stack creation to complete. The status message \"CREATE_COMPLETE\" will soon appear next to \"dbgap\" on the list of stacks.\n\n\n\n\n\nThe best defense is a good offense, and you are encouraged to monitor traffic to clusters launched by the user. You may want to explore turning on \nVPC flow logs\n and \nCloudWatch alarms\n for suspicious activity.\n\n\nDelegate Elastic MapReduce and CloudFormation authorites to the new IAM user (administrator)\n\n\nThe new IAM user still needs sufficient privileges to run Rail-RNA on Elastic MapReduce.\n\n\n\n\nReturn to the \nAWS Console\n, again click \nIdentity and Access Management\n, but now click \nPolicies\n on the left sidebar.\n\n\n\n\nClick \nCreate Policy\n, then select \nCreate Your Own Policy\n. (You may need to click \nGet Started\n first.)\n\n\n\n\nUnder \nPolicy Name\n, enter \"UseExistingEMRRoles\".\n\n\n\n\nUnder \nPolicy Document\n, paste the following.\n\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                 \"iam:GetInstanceProfile\",\n                 \"iam:GetRole\",\n                 \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n\n\n\n\n\n\nClick \nCreate Policy\n.\n\n\n\n\n\nNow click \nUsers\n in the left pane, select the new IAM user, and click the \nPermissions\n tab.\n\n\n\nClick \nAttach Policy\n, and select the \nAWSCloudFormationReadOnlyAccess\n, \nAmazonElasticMapReduceFullAccess\n, and \nUseExistingEMRRoles\n policies. Then click Attach Policy.\n\n\nDifferent policies including only some of the permissions from these may be included, but note that the user must be able to:\n\n\nlaunch Elastic MapReduce clusters into the VPC from the secure dbGaP CloudFormation stack created by the administrator above, and\n\n\nread and write to the secure S3 bucket created by the administrator on behalf of the user.\n\n\n\n\n\n\n\n\n\n\n\n\nSet up default EMR roles (administrator \n user)\n\n\n\n\nAdministrator:\n follow \nthese instructions\n to create default roles for Elastic MapReduce.\n\n\nUser:\n run\naws emr create-default-roles --profile dbgap\n\n\n\nto retrieve the default Elastic MapReduce roles created by the administrator.\n\n\n\n\n\n\nTest the configuration (user)\n\n\ndbGaP support has kindly provided a dataset composed of public RNA-seq samples from \n1000 Genomes\n exclusively for testing secure cloud-based pipelines. Its project accession number on SRA is \nSRP041052\n. The user may test their configuration on the lymphoblastoid cell line sample \nSRR1219818\n from that project by following these instructions.\n\n\n\n\nDownload \nthe dbGaP repository key\n for the test data. The key is referenced as \n/path/to/prj_phs710EA_test.ngc\n here.\n\n\nRun\nrail-rna go local\n  -a hg38\n  -o s3://this-is-a-bucket-name-the-user-names-up/dbgaptest\n  -c 1\n  -m https://raw.githubusercontent.com/nellore/rail/master/ex/secure.manifest\n  --secure-stack-name dbgap\n  --profile dbgap\n  --dbgap-key /path/to/prj_phs710EA_test.ngc\n\n\n\n\n\n\n\nto submit a secure job flow into the public subnet of the VPC created above that preprocesses and aligns the test sample. Use the EMR interface to monitor the progress of the job flow, and check the bucket \ns3://this-is-a-bucket-name-the-user-names-up/dbgaptest\n for results after it's done.\n\n\nAnalyze dbGaP-protected data\n\n\nThe user may now submit Rail-RNA jobs that analyze dbGaP-protected data from their computer. A line in a manifest file (as described in the \ntutorial\n and \nreference\n) corresponding to a dbGaP-protected sample has the following format.\n\n\ndbgap:\nSRA run accession number\n(tab)0(tab)\nsample label\n\n\n\n\n\nwhere a run accession number from SRA begins with \nSRR\n, \nERR\n, or \nDRR\n. An example manifest file is the \ntest manifest file\n used in the previous section. Every Rail-RNA command analyzing dbGaP data should include the command-line parameters \n--secure-stack-name dbgap --profile dbgap --dbgap-key [the key file with the NGC extension you download]\n and should write to the secure bucket created by the administrator. An example command follows.\n\n\nrail-rna go elastic\n  -m dbgap.manifest\n  -a hg38\n  -o s3://this-is-a-bucket-name-the-user-makes-up/dbgapout\n  -c 1\n  --secure-stack-name dbgap\n  --profile dbgap\n  --dbgap-key /path/to/some_dbgap_key.ngc\n\n\n\n\nRail-RNA does not currently support analyzing TCGA data.\n\n\nHelpful notes for administrators\n\n\nAs for any new AWS account, you should consider how you would like to configure billing.  \nConsolidated billing\n can be convenient if you are managing multiple AWS accounts simultaneously.\n\n\nFurther, you should consider raising your EC2 instance limits.  This is particularly important if you plan to analyze large datasets (more than 100 RNA-seq samples at a time).  To raise your limits, visit \nthis page\n.", 
            "title": "dbGaP on EMR"
        }, 
        {
            "location": "/dbgap/#securely-analyze-dbgap-protected-data-in-the-cloud", 
            "text": "The  National Institutes of Health (NIH)  maintains security  requirements  and  recommmendations  for analyzing controlled-access genomic data, including  dbGaP -protected data. With some setup beyond installation, Rail-RNA can analyze dbGaP-protected RNA-seq data on Amazon Elastic MapReduce in a way that complies with these policies.  Amazon Web Services  (AWS) has also released complementary documents on securing its resources, including some  security best practices  and the whitepaper  Architecting for Genomic Data Security and Compliance in the Cloud .  Rail-RNA ensures encryption of all data it handles at rest---on the Elastic MapReduce cluster and on S3---as well as in transit. Rail-RNA also ensures that the Elastic MapReduce cluster is sufficiently isolated from the internet by launching all job flows into a  Virtual Private Cloud  (VPC) augmented by several security features. (See  this section  for more information.) The steps below create a new  AWS IAM  account especially for analyzing dbGaP-protected data. To perform these steps, both user and AWS site administrator should be available. (For many investigators, user and administrator will be the same person.) It is recommended that they are physically together to minimize passing of credentials. The user should already have Rail-RNA  installed  on their local machine, including the  AWS Command Line Interface (CLI) . The user should also have  requested access  to some dbGaP-protected sample on the  Sequence Read Archive  (SRA) and received a key file with an  ngc  extension.  Analysis of data on  CGHub  is currently unsupported, but we hope to add support in the near future.  Set up an administrator account (administrator)  These steps should be performed if the site administrator is new to AWS.   Navigate to  http://aws.amazon.com/free  in your web browser.  Click  Create a free account .  Check the  I am a new user  box and and continue to follow the instructions to create your new account. You'll enter, for example, contact and payment information. Note that the  Basic  level of service is sufficient for using Rail-RNA.  Make a note of your account number.  Log into the [AWS console] using the new account's email address and password.  Click on the arrow next to your user name in the gray banner at the top of the page.  Select  My Account , and the  Account Id  will be displayed at the top of the page.    Secure the account  Log into the  AWS console  using the new account's email address and password.  Open the  Identity and Access Management  page.  Under  Security Status , click  Activate MFA on your root account , then click  Manage MFA , and follow the instructions to enable multi-factor authentication. We use a virtual MFA device (smartphone) with Google Authenticator.\n   Under  Apply an IAM password policy , click  Manage Password Policy . \nConfigure the password policy according to the requirements mentioned in the  NIH Security Best Practices for Controlled-Access Data Subject to the NIH Genomic Data Sharing (GDS) Policy . This usually entails the following, but please note that your institution may impose more stringent requirements:  Requiring a minimum password length of 12  Requiring at least one uppercase letter  Requiring at least one lowercase letter  Requiring at least one number  Requiring at least one non-alphanumeric character  Enable password expiration after 120 days    Click  Apply password policy .     Set up a new IAM user (administrator   user)  During this process, it is best for the account administrator to sit with the user to minimize passing credentials.     Administrator:  create new IAM user.   From the new user's computer, log into the  AWS Console  and select  Identity and Access Management .  Click  Users  on the left pane, then  Create New Users  on the right pane.  Enter the new user's username. We call the new user  dbgapuser  in the screenshot. Check the  Generate an access key for each user  checkbox, and click  Create .  Click  Download Credentials . These credentials ( credentials.csv ) include the AWS Access Key ID and AWS Secret Access Key. It is recommended that the file containing the credentials be made readable only by the user immediately. The credentials should never be shared, intentionally or inadvertently, with anyone else.     User:  register credentials with the AWS CLI by entering  aws configure --profile dbgap  at a terminal prompt on the user's computer. Enter the AWS Access Key ID, AWS Secret Access Key, and a default region as prompted. We recommend using the  us-east-1  because its connection to dbGaP-protected data on SRA appears to be fastest. A default output format need not be specified. Now the new user can issue AWS API calls via the AWS CLI.  It is recommended that credentials file that was just downloaded is now deleted.    Administrator:  Set user's password.   Return to the  AWS Console , again click  Identity and Access Management , again click  Users  on the left sidebar, and select the new user. Under  User Actions , click  Manage Password .  Select  Assign an auto-generated password , check the  Require user to create a new password at next sign-in  box, and click  Apply .  Click  Download Credentials . The new credentials file  credentials (2).csv  contains the username, the auto-generated password, and the URL for the account-specific login page.     User:  navigate to the login page URL from  credentials (2).csv , log in, and change the password as prompted.     Create a secure CloudFormation stack (administrator)  CloudFormation  facilitates creation and management of a group of related AWS resources. Rail-RNA is bundled with a CloudFormation template for creating a  Virtual Private Cloud  (VPC) with a  single public subnet . A Rail-RNA job flow that analyzes dbGaP data is launched into this subnet. The VPC is supplemented by several security features, including   a VPC endpoint for S3, which ensures that the connection between the Elastic MapReduce cluster and S3 is private.  security groups that block all inbound traffic to the cluster from the internet except from the Elastic MapReduce webservice.  the creation of a secure bucket on S3 into which Rail-RNA should write all its output when operating on dbGaP-protected data. The bucket has an attached policy barring uploads that do not have server-side encryption (AES256) turned on.  CloudTrail  logs recording AWS API calls. These are written to the secure bucket.   The user will find the CloudFormation template at  $RAILDOTBIO/cloudformation/dbgap.template  and can send it to the administrator, but it is recommended that the administrator grab the latest version of the template  here . Implement it by following these steps. (If the administrator already has CloudTrail turned on, they may not work, causing a rollback. An administrator satisfied with their CloudTrail configuration may instead want to use  this alternative CloudFormation template , which creates the VPC but does not attempt to set up CloudTrail.)   Click  CloudFormation  in the AWS console, making sure the region in the upper-right corner of the screen is the same as the user's default region (typically  us-east-1 , i.e., N. Virginia).  Click  Create Stack .  Under  Choose a template , opt to upload  dbgap.template  to Amazon S3, and click  Next .  On the next screen:  Next to  Stack name , write \"dbgap\".  Next to  Parameters , let the user type the name of a secure bucket into which they will write all of Rail-RNA's output. The bucket name should not have been taken by any other S3 user.    Click  Next  and  Next  again, then click  Create  and wait for the stack creation to complete. The status message \"CREATE_COMPLETE\" will soon appear next to \"dbgap\" on the list of stacks.   The best defense is a good offense, and you are encouraged to monitor traffic to clusters launched by the user. You may want to explore turning on  VPC flow logs  and  CloudWatch alarms  for suspicious activity.  Delegate Elastic MapReduce and CloudFormation authorites to the new IAM user (administrator)  The new IAM user still needs sufficient privileges to run Rail-RNA on Elastic MapReduce.   Return to the  AWS Console , again click  Identity and Access Management , but now click  Policies  on the left sidebar.   Click  Create Policy , then select  Create Your Own Policy . (You may need to click  Get Started  first.)   Under  Policy Name , enter \"UseExistingEMRRoles\".   Under  Policy Document , paste the following.  {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                 \"iam:GetInstanceProfile\",\n                 \"iam:GetRole\",\n                 \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}    Click  Create Policy .   Now click  Users  in the left pane, select the new IAM user, and click the  Permissions  tab.  Click  Attach Policy , and select the  AWSCloudFormationReadOnlyAccess ,  AmazonElasticMapReduceFullAccess , and  UseExistingEMRRoles  policies. Then click Attach Policy. \nDifferent policies including only some of the permissions from these may be included, but note that the user must be able to:  launch Elastic MapReduce clusters into the VPC from the secure dbGaP CloudFormation stack created by the administrator above, and  read and write to the secure S3 bucket created by the administrator on behalf of the user.       Set up default EMR roles (administrator   user)   Administrator:  follow  these instructions  to create default roles for Elastic MapReduce.  User:  run aws emr create-default-roles --profile dbgap  to retrieve the default Elastic MapReduce roles created by the administrator.    Test the configuration (user)  dbGaP support has kindly provided a dataset composed of public RNA-seq samples from  1000 Genomes  exclusively for testing secure cloud-based pipelines. Its project accession number on SRA is  SRP041052 . The user may test their configuration on the lymphoblastoid cell line sample  SRR1219818  from that project by following these instructions.   Download  the dbGaP repository key  for the test data. The key is referenced as  /path/to/prj_phs710EA_test.ngc  here.  Run rail-rna go local\n  -a hg38\n  -o s3://this-is-a-bucket-name-the-user-names-up/dbgaptest\n  -c 1\n  -m https://raw.githubusercontent.com/nellore/rail/master/ex/secure.manifest\n  --secure-stack-name dbgap\n  --profile dbgap\n  --dbgap-key /path/to/prj_phs710EA_test.ngc    to submit a secure job flow into the public subnet of the VPC created above that preprocesses and aligns the test sample. Use the EMR interface to monitor the progress of the job flow, and check the bucket  s3://this-is-a-bucket-name-the-user-names-up/dbgaptest  for results after it's done.  Analyze dbGaP-protected data  The user may now submit Rail-RNA jobs that analyze dbGaP-protected data from their computer. A line in a manifest file (as described in the  tutorial  and  reference ) corresponding to a dbGaP-protected sample has the following format.  dbgap: SRA run accession number (tab)0(tab) sample label   where a run accession number from SRA begins with  SRR ,  ERR , or  DRR . An example manifest file is the  test manifest file  used in the previous section. Every Rail-RNA command analyzing dbGaP data should include the command-line parameters  --secure-stack-name dbgap --profile dbgap --dbgap-key [the key file with the NGC extension you download]  and should write to the secure bucket created by the administrator. An example command follows.  rail-rna go elastic\n  -m dbgap.manifest\n  -a hg38\n  -o s3://this-is-a-bucket-name-the-user-makes-up/dbgapout\n  -c 1\n  --secure-stack-name dbgap\n  --profile dbgap\n  --dbgap-key /path/to/some_dbgap_key.ngc  Rail-RNA does not currently support analyzing TCGA data.  Helpful notes for administrators  As for any new AWS account, you should consider how you would like to configure billing.   Consolidated billing  can be convenient if you are managing multiple AWS accounts simultaneously.  Further, you should consider raising your EC2 instance limits.  This is particularly important if you plan to analyze large datasets (more than 100 RNA-seq samples at a time).  To raise your limits, visit  this page .", 
            "title": "Securely analyze dbGaP-protected data in the cloud"
        }, 
        {
            "location": "/reference/", 
            "text": "Command-line usage\n\n\nRail-RNA is invoked by entering\n\n\nrail-rna \njob flow\n \nmode\n \n[args]\n\n\n\n\n\n. \njob flow\n can be \nprep\n, \nalign\n, or \ngo\n; \ngo\n executes the \nprep\n and \nalign\n job flows in succession; \nmode\n can be \nlocal\n, \nparallel\n, or \nelastic\n. To get help for a given combination of job flow and mode, enter\n\n\nrail-rna \njob flow\n \nmode\n -h\n\n\n\n\n. Command-line arguments (args) are described in detail below.\n\n\nRequired args\n\n\n-m/--manifest \nfile\n\n\nThis is the path to a Rail-RNA manifest file. Its format is described in \nTutorial\n. Each line corresponds to a different RNA-seq sample. A line for a single-end sample looks like this---\n\n\nFASTQ/FASTA URL\n(tab)\nURL MD5 checksum or 0\n(tab)\nsample label\n\n\n\n\n\n---while a line for a paired-end sample looks like this---\n\n\nFASTQ/FASTA URL 1\n(tab)\nURL 1 MD5 checksum or 0\n(tab)\nFASTQ/FASTA URL 2\n(tab)\nURL 2 MD5 checksum or 0\n(tab)\nsample label\n\n\n\n\n\n. For non-dbGaP-protected SRA samples, a line looks like this---\n\n\nsra:\nSRA run accession number\n(tab)0(tab)\nsample label\n\n\n\n\n\n--while for dbGaP-protected SRA samples, a line looks like this---\n\n\ndbgap:\nSRA run accession number\n(tab)0(tab)\nsample label\n\n\n\n\n\n.\n\n\n-x/--bowtie-idx/ \nidx,idx | idx\n\n\n(\nlocal\n and \nparallel\n modes only)\n\n\nRail-RNA needs the paths to both a Bowtie 1 index and a Bowtie 2 index of exactly the same reference FASTA. Only the basename path(s) should be specified. There are six Bowtie index files; their extensions are \n.1.ebwt\n, \n.2.ebwt\n, \n.3.ebwt\n, \n.4.ebwt\n, \n.rev.1.ebwt\n, and \n.rev.2.ebwt\n. Similarly, there are six Bowtie 2 index files; their extensions are \n.1.bt2\n, \n.2.bt2\n, \n.3.bt2\n, \n.4.bt2\n, \n.rev.1.bt2\n, \n.rev.2.bt2\n. If the Bowtie 1 and Bowtie 2 index files are in the same directory and have the same basename, only the path to this basename need be specified, like so:\n\n\n-x \npath to common Bowtie index basename\n\n\n\n\n\n. Otherwise, paths to both basenames should be specified, like so:\n\n\n-x \npath to Bowtie 1 index basename\n \npath to Bowtie 2 index basename\n\n\n\n\n\n. Rail-RNA is forgiving of your specifing Bowtie indexes in the reverse order.\n\n\n-c/--core-instance-count \nint\n\n\n(\nelastic\n mode only)\n\n\nThis is the number of core instances Rail-RNA should reserve to execute the requested job flow (one of \nprep\n, \nalign\n, or \ngo\n). The instance type is by default the same as the \n--master-instance-type\n. Rail-RNA uses an Elastic MapReduce cluster with one master instance and \nint\n core instances.\n\n\n-a/--assembly \nchoice | tgz\n\n\n(\nelastic\n mode only)\n\n\nThis is the assembly to which Rail-RNA should align the data specified in \n--manifest/-m\n. Right now, only \nhg19\n is supported natively, but you can \nbuild Bowtie indexes\n of an assembly, compress them into a \ntar.gz\n, upload the archive to S3, and then specify its full path \ntgz\n. The basename of both Bowtie indexes should be \ngenome\n. In more detail, the archive should have the following contents.\n\n\nindex/genome.1.bt2\nindex/genome.2.bt2\nindex/genome.3.bt2\nindex/genome.4.bt2\nindex/genome.rev.1.bt2\nindex/genome.rev.2.bt2\nindex/genome.1.ebwt\nindex/genome.2.ebwt\nindex/genome.3.ebwt\nindex/genome.rev.1.ebwt\nindex/genome.rev.2.ebwt\n\n\n\n\nNote that \nindex/genome.4.ebwt\n is missing above. It's exactly the same as the Bowtie 2 index file \ngenome.4.bt2\n, and it should not be included in the archive. If you've uploaded the index to, say, \ns3://my-bucket/genome.tgz\n, you'd specify it at the command line as\n\n\n-a s3://my-bucket/genome.tgz\n\n\n\n\n.\n\n\nGeneral options\n\n\nThese are parameters that apply generally to Rail-RNA job flows.\n\n\n-h/--help\n\n\nThis is the go-to command-line parameter for figuring out how to use Rail-RNA without whatever you're reading now.\n\n\nBoolean parameter; has no argument.\n\n\n-v/--version\n\n\nWhen you \nask for help\n from us, we'll want to know the version of Rail-RNA you've been using. Include the output of\n\n\nrail-rna -v\n\n\n\n\nin your support request.\n\n\nBoolean parameter; has no argument.\n\n\n-j/--json\n\n\nRail-RNA job flows are encoded as JSON objects. A job flow's JSON is interpreted and executed by \nDooplicity's EMR simulator\n in \nlocal\n and \nparallel\n modes and Amazon Elastic MapReduce via \nDooplicity's EMR runner\n in \nelastic\n mode. You can view and hack the JSON file to, say, continue a failed job flow. If you'd like to view the job flow JSON constructed by Rail-RNA without executing it, use the \n-j/--json\n command-line parameter.\n\n\nBoolean parameter; has no argument.\n\n\n--profile \nstr\n\n\nThis is the \nAWS CLI profile\n from which Rail-RNA should pull your credentials when communicating with Amazon Web Services. It's typically relevant only in \nelastic\n mode.\n\n\nDefault: credentials are pulled from the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, but if those aren't present, the credentials are instead pulled from the default AWS CLI profile.\n\n\n-f/--force\n\n\nBy default, Rail-RNA warns you in all modes if there are existing output files present at the destination specified by \n-o/--output\n. Invoking \n-f/--force\n overwrites all output directories if they exist. This also includes all files in subdirectories of the log directory in \nlocal\n and \nparallel\n modes, and all files in the intermediate directory in \nelastic\n mode.\n\n\nUnless you'll need to debug job flows, you won't miss any of these files, so feel free to force.\n\n\nBoolean parameter; has no argument.\n\n\n--verbose\n\n\nIf you're having trouble with a job flow in Rail, it may help to invoke \n--verbose\n to write more details to task logs. \nTutorial\n covers an example in local mode that reviews one such log.\n\n\nBoolean parameter; has no argument.\n\n\n--do-not-bin-quals\n\n\n(\nprep\n and \ngo\n job flows only)\n\n\nBy default, Rail-RNA places a Phred quality score from the quality sequence for each input record of a FASTQ file into one of five bins. The formula for binning mirrors \nBowtie 2's default formula\n and thus does not compromise alignment quality. Binning quality scores does save space and speed up job flows, however, especially in \nelastic\n mode, where Rail-RNA always compresses intermediate data.\n\n\nThis option leaves quality sequences intact.\n\n\nBoolean parameter; has no argument.\n\n\n--short-read-names\n\n\n(\nprep\n and \ngo\n job flows only)\n\n\nSAM/BAM file outputs are sometimes considerably larger simply because read names are large. This option kills read names in favor of short yet still unique identifiers.\n\n\nBoolean parameter; has no argument.\n\n\n--skip-bad-records\n\n\n(\nprep\n and \ngo\n job flows only)\n\n\nBy default, if an input record from a FASTA/FASTQ file is invalid, Rail-RNA's preprocessing step chokes, as \nreviewed in Tutorial\n. Use this parameter to instead be robust to bad records in input files and skip them. Bad records typically occur when an input file was somehow truncated or otherwise corrupted.\n\n\nBoolean parameter; has no argument.\n\n\n--do-not-check-manifest\n\n\n(\nprep\n and \ngo\n job flows only)\n\n\nRail-RNA's default behavior is to check for the existence of each input file in the manifest file before starting a job flow. This can be time-consuming for a set of input files on a remote host. If you know the files are accessible, \n--do-not-check-manifest\n turns the default behavior off.\n\n\nBoolean parameter; has no argument.\n\n\n--log \ndir\n\n\n(\nlocal\n and \nparallel\n modes only)\n\n\ndir\n is used both for aggregating intermediate files between steps of job flows and for storing log files for diagnosing problems.\n\n\nDefault: \"rail-rna_logs\" in the current working directory\n\n\n--intermediate \ns3_dir/hdfs_dir\n\n\n(\nelastic\n mode only)\n\n\nThis is a directory in an S3 bucket or on the temporary HDFS of an Elastic MapReduce cluster where intermediate data across all workers is aggregated. If using S3, \ns3_dir\n should begin with \ns3://some-bucket\n. If using HDFS, \nhdfs_dir\n should begin with \nhdfs:///\n. Use S3 and set \n--intermediate-lifetime\n to -1 to keep intermediates.\n\n\nDefault: argument of \n-o/--output\n with the string \".intermediate\" tacked on\n\n\n-p/--num-processes \nint\n\n\n(\nlocal\n mode only)\n\n\nRail-RNA runs your job flow on up to \nint\n processes simultaneously.\n\n\nDefault: if you have more than one processing core available, Rail-RNA runs on as many as you have less one so it's not a resource hog. If you have only one processing core, Rail-RNA runs on it.\n\n\n--scratch \ndir\n\n\n(\nlocal\n and \nparallel\n modes only)\n\n\nThis is a directory used for storing temporary files accumulated when aggregating intermediate data. In \nparallel\n mode, this should always be in some node-local directory like \n/tmp\n.\n\n\nDefault: securely created temporary directory created by Python's \ntempfile.mkdtemp()\n\n\n--keep-intermediates\n\n\n(\nlocal\n and \nparallel\n modes only)\n\n\nRail-RNA's default behavior is to delete aggregated intermediate data as soon as the job flow it's running no longer needs them. Keeping them around after a job flow is complete using \n--keep-intermediates\n is sometimes useful for diagnosing problems, however.\n\n\nBoolean parameter; has no argument.\n\n\n-g/--gzip-intermediates\n\n\n(\nlocal\n and \nparallel\n modes only)\n\n\nRail-RNA has a small memory footprint but a large storage footprint. When space is tight, the storage footprint can be minimized in \nlocal\n and \nparallel\n modes by compressing intermediates with the \n-g/--gzip-intermediates\n command-line parameter. This will slow down the job flow a bit.\n\n\nBoolean parameter; has no argument.\n\n\n--gzip-level \nint\n\n\n(\nlocal\n and \nparallel\n modes only)\n\n\nThis is the level of gzip compression to use for intermediate data if it's being compressed. The extremes are 1, meaning no compression, and 9, meaning high compression. Values closer to 9 increase the time a given job flow takes significantly.\n\n\nDefault: 3\n\n\n-r/--sort-memory-cap \ndec\n\n\n(\nlocal\n and \nparallel\n modes only)\n\n\nRail-RNA uses \nGNU Coreutils\n \nsort\n when aggregating intermediate data. \ndec\n is the maximum amount of memory (in bytes) used by \nsort\n per process\n\n\nDefault: 307200\n\n\n--nucleotides-per-input \nint\n\n\n(\nlocal\n and \nparallel\n modes only)\n\n\nAfter the \nprep\n job flow downloads a sample's FASTA/FASTQs from a remote server, it assigns reads to alignment tasks. Each task may be executed by a different worker. \nint\n is the maximum number of nucleotides from input reads to assign to any single task. Smaller values of \nint\n increase parallelism in the following alignment step. This command-line parameter does not apply to input data stored locally, which Rail-RNA attempts to distribute evenly among workers.\n\n\nDefault: 100000000\n\n\n--do-not-gzip-input\n\n\n(\nlocal\n and \nparallel\n modes only)\n\n\nBy default, Rail-RNA gzips preprocessed input data, which is \nnot\n considered intermediate data. Turning this option on leaves preprocessed input reads uncompressed.\n\n\n--max-task-attempts \nint\n\n\nSometimes, a worker attempting a task will fail for miscellaneous reasons. For example, the node on which the worker is operating may transiently run out of space. Hadoop clusters are resilient to failures like this because they automatically reattempt tasks. Rail-RNA can reattempt tasks in \nlocal\n and \nparallel\n modes, too, when it's not using Hadoop. \n--max-task-attempts\n applies in all modes: a task is reattempted up to \nint\n times by different workers in the cluster, if possible, before failing a job flow. On Elastic MapReduce, which uses Hadoop, \nint\n specifies the config parameters \nmapreduce.map.maxattempts\n and \nmapreduce.reduce.maxattempts\n.\n\n\nAlgorithm options\n\n\nThese are parameters that tweak Rail-RNA's algorithms.\n\n\n--bowtie2-args \nstr\n\n\nThese are arguments to pass to Bowtie 2, and they should be enclosed in\ndouble-quotes. A couple arguments are always ignored, however.\n\n\n\n\nBowtie 2 is always run in \n--local\n mode by Rail-RNA, so \n--end-to-end\n won't be passed to it.\n\n\nRail-RNA bins quality scores in a particular way, and \n--mp\n arguments won't be passed to Bowtie 2.\n\n\n\n\nDefault: empty\n\n\n-k \nint\n\n\nTells Rail-RNA to look for up to \nint\n alignments per read per step. This takes precedence over any \n-k\n value specified in --bowtie2-args.\n\n\nDefault: no value; Rail-RNA looks for the best alignment.\n\n\n--isofrag-idx/-fx \nfile\n\n\nThis is a \ntar.gz file\n containing the transcript fragment (isofrag) index from previous Rail-RNA run. It is used in lieu of searching for exon-exon junctions. You'll find an \nisofrags.tar.gz\n file with the isofrag index in the \ncross_sample_outputs/\n directory of a Rail-RNA run where \nidx\n wasone of the \ndeliverables\n, which is the default behavior.\n\n\nDefault: none; search for junctions.\n\n\n--partition-length \nint\n\n\nTo compute genome coverage by reads, Rail-RNA partitions the genome into bins of length \nint\n bases. Each bin is assigned to a different task to be executed by a different worker. The exon differentials described in our \npaper\n are summed within each bin to arrive at base-level coverage.\n\n\nPartition lengths that are too large decrease parallelism and contribute to load imbalance, but partition lengths that are too small (tens or hundreds of bases) make for larger intermediate data. It's probably not necessary to change the default value.\n\n\nDefault: 5000\n\n\n--max-readlet-size \nint\n\n\nRail-RNA divides reads into short overlapping segments called readlets and subsequently aligns them to the genome to infer the presence of exon-exon junctions. For a complete description of the readletizing scheme Rail uses, see \nour paper\n. A readlet's size does not exceed \nint\n bases.\n\n\nFor human-size genomes, values between 22 and 27 work well, with smaller values improving sensitivity but decreasing specificity.\n\n\nDefault: 25\n\n\n--min-readlet-size \nint\n\n\nSee \n--max-readlet-size\n for a description of readlets. Keep \nint\n above 10 to avoid making too many false positive exon-exon junction calls.\n\n\nDefault: 15\n\n\n--readlet-interval \nint\n\n\nSee \n--max-readlet-size\n for a description of readlets. Here, \nint\n is the number of bases between the start positions of consecutive overlapping readlets. Increasing this value decreases sensitivity to novel exon-exon junctions. However, we haven't found that decreasing \n--readlet-interval\n below 4 increaes sensitivity much.\n\n\nDefault: 4\n\n\n--readlet-config-size \nint\n\n\nAfter finding exon-exon junctions in samples, Rail-RNA constructs a directed acyclic graph (DAG) whose nodes represent introns. Each edge has weight equal to the number of exonic bases between the intron nodes it connects. Rail-RNA traverses the DAG to find all possible combinations of exon-exon junctions spanned by \nint\n exonic bases. It later aligns reads to transcript fragments spanning these combinations of exon-exon junctions.\n\n\nWe find a \n--readlet-config-size\n above 30 to be sufficient to resolve alignments spanning exon-exon junctions for human-size genomes.\n\n\nDefault: 35\n\n\n--max-intron-size \nint\n\n\nThis parameter has Rail filter out introns that span more than \nint\n bases. It significantly reduces false positive exon-exon junction calls. There are only a handful of introns that exceed 500,000 bases in the human genome, which explains the default.\n\n\nDefault: 500000\n\n\n--min-intron-size \nint\n\n\nThis parameter has Rail filter out introns spanning fewer than \nint\n bases. Making this too small might have Rail mistake insertions for introns.\n\n\nDefault: 10\n\n\n---min-exon-size \nint\n\n\nThis sets various parameters across Rail-RNA that enforce its sensitivity to exons spanning only at least \nint\n bases. Most conspicuously:\n\n\n\n\nWhen searching for exon-exon junctions with readlet alignments, Rail-RNA will look for small exons that span at least \nint\n bases. Making \nint\n smaller increases false positive junction calls.\n\n\nWhen traversing the DAG described under \n--readlet-config-size\n, Rail-RNA will eliminate junction combinations spanning exons below \nint\n.\n\n\n\n\nDefault: 9\n\n\n--search-filter \nchoice/int\n\n\nRail-RNA aligns all read sequences to the genome first with Bowtie 2 in \n--local\n mode, which can soft-clip base sequences at read ends. Ordinarily, all read sequences with soft-clipped bases are divided into readlets and later searched for exon-exon junctions---but instead, the read sequences searched for junctions may be chosen based on how many if its bases are soft-clipped in its primary alignment instead. If \nchoice/int\n is an integer \nint\n, a read sequence is only searched for junctions if \n= \nint\n of its bases are soft-clipped. \nchoice/int\n can also be \nchoice\n from {\nstrict\n, \nmild\n, \nnone\n}. If \nchoice\n is \nstrict\n, \nint\n is set equal to \n--min-exon-size\n; if \nchoice\n is \nmild\n, \nint\n is 2/3 * \n--min-exon-size\n; and if \nchoice\n is \nnone\n, \nint\n is 1.\n\n\nDefault: none; i.e., 1\n\n\n--junction-criteria \ndec,int\n\n\nWhen Rail-RNA analyzes more than one RNA-seq sample, it accumulates a master list of exon-exon junctions across samples. This master list is used to construct transcript fragments to which reads are realigned. However, when hundreds of samples are analyzed, the master list typically blows up: true positive junction calls tend to be shared by many samples, while false positive junction calls tend to be unique to small numbers of samples. As the number of samples analyzed increases, so does the master list of junctions, and false positive calls make up a larger and larger fraction of the master list. False positive calls may be due to low-quality reads or repetitive/small exonic segments on either side of an intron that make it difficult to resolve the intron's proper start and end positions in the genome.\n\n\nThe \n--junction-criteria\n parameter \ndec,int\n filters out junctions that are not either present in at least a fraction \ndec\n of samples or detected in at least \nint\n reads of one sample. This way, only junctions that are detected from a small number of reads in any one sample or from a small number of samples are eliminated, and the quality of realigned reads is improved without significantly compromising sensitivity. (See \nour paper\n for an elaboration of this phenomenon.) For example, specifying \n--junction-criteria 0.02,7\n or \n--junction-criteria 0.02 7\n filters out junctions that are found in fewer that 2% of samples and are detected in fewer in 7 reads in any one sample before realignment.\n\n\nDefault: 0.05,5\n\n\n--normalize-percentile \ndec\n\n\nNormalization factors for coverage of the genome by primary alignments and by uniquely aligning reads are output by Rail-RNA if the \ntsv\n deliverable is requested; see the \nappropriate section of Deliverables\n. The normalization factor of a given sample can be obtained by\n\n\n\n\nCounting the number n of bases that are covered by at least k reads, where k \n= 1.\n\n\nWriting a list of numbers where each value k occurs exactly n times.\n\n\nSorting the list constructed in step 2 in increasing order.\n\n\nSelecting the \ndec\n*100th-percentile value from the list.\n\n\n\n\nWhen \ndec\n is 0.75, this is \nupper-quartile normalization\n.\n\n\nDefault: 0.75\n\n\n--do-not-drop-polyA-tails\n\n\nBy default Rail-RNA, does not map reads for which all bases to the left or right of a segment spanning \n--min-exon-size\n bases \n\n\nBoolean parameter; has no argument.\n\n\n--tie-margin \nint\n\n\nOrdinarily, two alignments of a read are considered to have \"tied\" alignment scores if and only if the scores are exactly equal, and \nint\n is 0. However, for the purpose of identifying uniquely aligning reads and resolving primary alignments based on coverage, toggling this parameter permits a score difference of \nint\n per 100 bases among alignments whose scores are considered ties. For example, 150\nand 144 are tied alignment scores for a 100-bp read when --tie-margin is 6, and if a read has alignments with both scores, it is not considered uniquely aligned.\n\n\nDefault: 0\n\n\n--library-size \nint\n\n\nAs described in \nDeliverables\n, in the \ncross_sample_outputs\n directory, \n[mean|median].\nchr\n.bw\n encodes the [mean|median] coverage at each base across samples on chromosome \nchr\n by primary alignments, and \n[mean|median].\nchr\n.bw\n encodes the [mean|median] coverage at each base on chromosome \nchr\n by uniquely aligning reads. To normalize each sample's contribution to a given [mean|median], the number of reads covering a given base of the genome is multiplied by \nint\n*1,000,000 and divided by the number of mapped reads in the sample. So \nint\n is a standard library size in millions of reads that permits easier interpretation of the mean and median coverage bigWigs.\n\n\nDefault: 40\n\n\nOutput options\n\n\n-o/--output \ndir\n\n\nRail-RNA will write its output to the directory \ndir\n. In \nlocal\n mode, \ndir\n can be on either the local filesystem or on S3. In \nparallel\n mode, \ndir\n can be on S3 or at some path accessible to all nodes in your IPython cluster. In \nelastic\n mode, \ndir\n must be on S3.\n\n\nDefault: \n./rail-rna_out\n in \nlocal\n and \nparallel\n modes; a required parameter in \nelastic\n mode\n\n\n-d/--deliverables \nchoice,...\n\n\nThis command-line option permits suppressing some outputs of the full Rail-RNA pipeline. Check out \nDeliverables\n to learn more.\n\n\n--drop-deletions\n\n\nBy default, Rail-RNA counts bases deleted from the reference in a read alignment as covered by the read when computing the coverage vectors stored in bigWigs. Turning this option adjusts Rail-RNA's behavior so such deleted bases do not contribute to the coverage vectors.\n\n\nBoolean parameter; has no argument.\n\n\n--do-not-output-bam-by-chr\n\n\nBy default, Rail-RNA outputs alignment BAMs by chromosome/sample to increase parallelism in a terminal output step. You can output BAMs by sample alone with this option.\n\n\nBoolean parameter; has no argument.\n\n\n--do-not-output-ave-bw-by-chr\n\n\nObtaining a bigWig storing average (mean or median) coverage across many samples can involve much more computation than obtaining a bigWig storing coverage for a single sample. Moreover, an average coverage bigWigs can take up considerably more space than a single-sample bigWig. To mitigate the resulting load balance issues, Rail-RNA outputs average coverage bigWigs chromosome by chromosome, by default. You can output each average coverage bigWig for the entire genome instead with this option.\n\n\nBoolean parameter; has no argument.\n\n\n--indel-criteria \ndec,int\n\n\nAccumulating indels across hundreds of samples bloats the \ninsertions.tsv.gz\n and \ndeletions.tsv.gz\n files with many false positives for which there is little evidence. The \n--indel-criteria\n parameter \ndec,int\n does not write indels to these cross-sample output files unless they are either present in at least a fraction \ndec\n of samples or detected in at least \nint\n reads of one sample. \n--indel-criteria\n's usage is similar to that of \n--junction-criteria\n.\n\n\nDefault: \n0.05,5\n\n\nElastic MapReduce options\n\n\nThese are arguments that pertain to working with Amazon Web Services in Rail-RNA's \nelastic\n mode.\n\n\n--intermediate-lifetime \nint\n\n\nIt costs money to keep files around on Simple Storage Service (S3). To help save you money, Rail-RNA schedules any directory in which it stores intermediate data on S3 for deletion after \nint\n days. If \nint\n is set to \n-1\n, no such scheduling is done, and intermediate data is kept for indefinitely long.\n\n\nCheck \nLifecycle\n under a given bucket's properties in the \nS3 console\n to view and adjust the rules Rail-RNA has made to delete intermediate data.\n\n\nDefault: 4\n\n\n--name \nstr\n\n\nIt can be useful to assign a distinctive name \nstr\n to an Elastic MapReduce job flow if you're working with many in the \nconsole\n.\n\n\nDefault: \nRail-RNA Job Flow\n\n\n--log-uri \ns3_dir\n\n\nThis is the directory on S3 to which Elastic MapReduce will ultimately copy Hadoop logs.\n\n\nDefault: argument of \n-o/--output\n with the string \".logs\" tacked on\n\n\n--ami-version \nstr\n\n\nThis is the version of the Amazon Machine Image to load onto each node of the Elastic MapReduce cluster. Changing \nstr\n is not recommended\n\n\nDefault: \n3.8.0\n\n\n--visible-to-all-users\n\n\nThis makes the Elastic MapReduce cluster accessible to all IAM users associated with the main Amazon Web Services account.\n\n\nBoolean parameter; has no argument.\n\n\n--action-on-failure \nchoice\n\n\nThis is the action Elastic MapReduce should take if the job flow it's running fails on a given step. \nchoice\n is one of {\nTERMINATE_JOB_FLOW\n, \nCANCEL_AND_WAIT\n, \nCONTINUE\n, \nTERMINATE_CLUSTER\n}.\n\n\nDefault: \nTERMINATE_JOB_FLOW\n\n\n--master-instance-count \nint\n\n\nThis is the number of master instances to include in your Elastic MapReduce cluster. It's only useful to include more than one in case a master instance dies; in general, \nint\n shouldn't be changed.\n\n\nDefault: 1\n\n\n--task-instance-count \nint\n\n\nTask instances are nodes in an Elastic MapReduce cluster that do not store data. Your job flow will continue even if they're lost, which can happen if nodes go down or--more likely--they're \nspot instances\n whose maximum bid price no longer exceeds the market prices. You should consider using task instances only if you'll be bidding for them on the spot market at the rate \n--task-instance-bid-price\n.\n\n\nDefault: 0\n\n\n--master-instance-bid-price \ndec\n\n\nIf you'd like to use the \nspot market\n to potentially save money on your job flow, you can set the bid price (in dollars/hour) for the master instance group here. Invoke this command-line parameter only if master instances should be spot.\n\n\nDefault: none; use on-demand master instances\n\n\n--core-instance-bid-price \ndec\n\n\nIf you'd like to use the \nspot market\n to potentially save money on your job flow, you can set the bid price (in dollars/hour) for the core instance group here. Invoke this command-line parameter only if core instances should be spot.\n\n\nDefault: none; use on-demand core instances\n\n\n--task-instance-bid-price \ndec\n\n\nIf you'd like to use the \nspot market\n to potentially save money on your job flow, you can set the bid price (in dollars/hour) for the task instance group here (if you have any task instances). Invoke this command-line parameter only if you are using task instances. Indeed, there is typically no point to task instances unless they're spot instances: you may as well use on-demand core instances and be afforded extra space.\n\n\n--master-instance-type \nchoice\n\n\nThis is the \ninstance type\n of the master instance group of your Elastic MapReduce cluster.\n\n\nDefault: \nc3.2xlarge\n\n\n--core-instance-type \nchoice\n\n\nThis is the \ninstance type\n of the core instance group of your Elastic MapReduce cluster.\n\n\nDefault: argument of \n--master-instance-type\n\n\n--task-instance-type \nchoice\n\n\nThis is the \ninstance type\n of the core instance group of your Elastic MapReduce cluster.\n\n\nDefault: argument of \n--master-instance-type\n if there are any task instances\n\n\n--ec2-key-name \nstr\n\n\nYou can specify an Elastic Compute Cloud (EC2) key pair name \nstr\n for SSHing to the master instance of your Elastic MapReduce cluster while your job flow is running.\n\n\nDefault: unspecified, which means you can't SSH to the master instance of your cluster\n\n\n--keep-alive\n\n\nThis option keeps an Elastic Mapreduce cluster alive when all its assigned steps are complete. Use \n--keep-alive\n in conjunction with \n--termination-protected\n and \n--ec2-key-name\n to be able to SSH to an Elastic MapReduce cluster and diagnose problems. However, you should make sure to terminate the cluster manually when you're finished.\n\n\nBoolean parameter; has no argument.\n\n\n--termination-protected\n\n\nThis option protects an Elastic MapReduce cluster from termination in case of step failure. Use \n--termination-protected\n in conjunction with \n--keep-alive\n and \n--ec2-key-name\n to be able to SSH an Elastic MapReduce cluster and diagnose problems. However, you should make sure to terminate the cluster manually when you're finished.\n\n\n--region \nchoice\n\n\nThis specifies the region in which your job flow will be run. Valid regions are given \nhere\n.\n\n\nDefault: the region from your \n--profile\n, but if that's unavailable, \nus-east-1\n (US Standard)\n\n\n--service-role \nstr\n\n\nYou should have set up your IAM service role by entering \naws emr create-default-roles\n after installing the AWS CLI. If for some reason related to permissioning you need to use a different IAM service role, specify its name as \nstr\n.\n\n\nDefault: taken from \n--profile\n if available; otherwise, attempts \nEMR_DefaultRole\n\n\n--instance-profile \nstr\n\n\nYou should have set up your IAM EC2 instance profile by entering \naws emr create-default-roles\n after installing the AWS CLI. If for some reason related to permissioning you need to use a different IAM EC2 instance profile, specify its name as \nstr\n.\n\n\nDefault: taken from \n--profile\n if available; otherwise, attempts \nEMR_EC2_DefaultRole", 
            "title": "Reference"
        }, 
        {
            "location": "/reference/#command-line-usage", 
            "text": "Rail-RNA is invoked by entering  rail-rna  job flow   mode   [args]   .  job flow  can be  prep ,  align , or  go ;  go  executes the  prep  and  align  job flows in succession;  mode  can be  local ,  parallel , or  elastic . To get help for a given combination of job flow and mode, enter  rail-rna  job flow   mode  -h  . Command-line arguments (args) are described in detail below.  Required args", 
            "title": "Command-line usage"
        }, 
        {
            "location": "/reference/#-m-manifest-file", 
            "text": "This is the path to a Rail-RNA manifest file. Its format is described in  Tutorial . Each line corresponds to a different RNA-seq sample. A line for a single-end sample looks like this---  FASTQ/FASTA URL (tab) URL MD5 checksum or 0 (tab) sample label   ---while a line for a paired-end sample looks like this---  FASTQ/FASTA URL 1 (tab) URL 1 MD5 checksum or 0 (tab) FASTQ/FASTA URL 2 (tab) URL 2 MD5 checksum or 0 (tab) sample label   . For non-dbGaP-protected SRA samples, a line looks like this---  sra: SRA run accession number (tab)0(tab) sample label   --while for dbGaP-protected SRA samples, a line looks like this---  dbgap: SRA run accession number (tab)0(tab) sample label   .", 
            "title": "-m/--manifest &lt;file&gt;"
        }, 
        {
            "location": "/reference/#-x-bowtie-idx-idxidx-idx", 
            "text": "( local  and  parallel  modes only)  Rail-RNA needs the paths to both a Bowtie 1 index and a Bowtie 2 index of exactly the same reference FASTA. Only the basename path(s) should be specified. There are six Bowtie index files; their extensions are  .1.ebwt ,  .2.ebwt ,  .3.ebwt ,  .4.ebwt ,  .rev.1.ebwt , and  .rev.2.ebwt . Similarly, there are six Bowtie 2 index files; their extensions are  .1.bt2 ,  .2.bt2 ,  .3.bt2 ,  .4.bt2 ,  .rev.1.bt2 ,  .rev.2.bt2 . If the Bowtie 1 and Bowtie 2 index files are in the same directory and have the same basename, only the path to this basename need be specified, like so:  -x  path to common Bowtie index basename   . Otherwise, paths to both basenames should be specified, like so:  -x  path to Bowtie 1 index basename   path to Bowtie 2 index basename   . Rail-RNA is forgiving of your specifing Bowtie indexes in the reverse order.", 
            "title": "-x/--bowtie-idx/ &lt;idx,idx | idx&gt;"
        }, 
        {
            "location": "/reference/#-c-core-instance-count-int", 
            "text": "( elastic  mode only)  This is the number of core instances Rail-RNA should reserve to execute the requested job flow (one of  prep ,  align , or  go ). The instance type is by default the same as the  --master-instance-type . Rail-RNA uses an Elastic MapReduce cluster with one master instance and  int  core instances.", 
            "title": "-c/--core-instance-count &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-a-assembly-choice-tgz", 
            "text": "( elastic  mode only)  This is the assembly to which Rail-RNA should align the data specified in  --manifest/-m . Right now, only  hg19  is supported natively, but you can  build Bowtie indexes  of an assembly, compress them into a  tar.gz , upload the archive to S3, and then specify its full path  tgz . The basename of both Bowtie indexes should be  genome . In more detail, the archive should have the following contents.  index/genome.1.bt2\nindex/genome.2.bt2\nindex/genome.3.bt2\nindex/genome.4.bt2\nindex/genome.rev.1.bt2\nindex/genome.rev.2.bt2\nindex/genome.1.ebwt\nindex/genome.2.ebwt\nindex/genome.3.ebwt\nindex/genome.rev.1.ebwt\nindex/genome.rev.2.ebwt  Note that  index/genome.4.ebwt  is missing above. It's exactly the same as the Bowtie 2 index file  genome.4.bt2 , and it should not be included in the archive. If you've uploaded the index to, say,  s3://my-bucket/genome.tgz , you'd specify it at the command line as  -a s3://my-bucket/genome.tgz  .  General options  These are parameters that apply generally to Rail-RNA job flows.", 
            "title": "-a/--assembly &lt;choice | tgz&gt;"
        }, 
        {
            "location": "/reference/#-h-help", 
            "text": "This is the go-to command-line parameter for figuring out how to use Rail-RNA without whatever you're reading now.  Boolean parameter; has no argument.", 
            "title": "-h/--help"
        }, 
        {
            "location": "/reference/#-v-version", 
            "text": "When you  ask for help  from us, we'll want to know the version of Rail-RNA you've been using. Include the output of  rail-rna -v  in your support request.  Boolean parameter; has no argument.", 
            "title": "-v/--version"
        }, 
        {
            "location": "/reference/#-j-json", 
            "text": "Rail-RNA job flows are encoded as JSON objects. A job flow's JSON is interpreted and executed by  Dooplicity's EMR simulator  in  local  and  parallel  modes and Amazon Elastic MapReduce via  Dooplicity's EMR runner  in  elastic  mode. You can view and hack the JSON file to, say, continue a failed job flow. If you'd like to view the job flow JSON constructed by Rail-RNA without executing it, use the  -j/--json  command-line parameter.  Boolean parameter; has no argument.", 
            "title": "-j/--json"
        }, 
        {
            "location": "/reference/#-profile-str", 
            "text": "This is the  AWS CLI profile  from which Rail-RNA should pull your credentials when communicating with Amazon Web Services. It's typically relevant only in  elastic  mode.  Default: credentials are pulled from the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, but if those aren't present, the credentials are instead pulled from the default AWS CLI profile.", 
            "title": "--profile &lt;str&gt;"
        }, 
        {
            "location": "/reference/#-f-force", 
            "text": "By default, Rail-RNA warns you in all modes if there are existing output files present at the destination specified by  -o/--output . Invoking  -f/--force  overwrites all output directories if they exist. This also includes all files in subdirectories of the log directory in  local  and  parallel  modes, and all files in the intermediate directory in  elastic  mode.  Unless you'll need to debug job flows, you won't miss any of these files, so feel free to force.  Boolean parameter; has no argument.", 
            "title": "-f/--force"
        }, 
        {
            "location": "/reference/#-verbose", 
            "text": "If you're having trouble with a job flow in Rail, it may help to invoke  --verbose  to write more details to task logs.  Tutorial  covers an example in local mode that reviews one such log.  Boolean parameter; has no argument.", 
            "title": "--verbose"
        }, 
        {
            "location": "/reference/#-do-not-bin-quals", 
            "text": "( prep  and  go  job flows only)  By default, Rail-RNA places a Phred quality score from the quality sequence for each input record of a FASTQ file into one of five bins. The formula for binning mirrors  Bowtie 2's default formula  and thus does not compromise alignment quality. Binning quality scores does save space and speed up job flows, however, especially in  elastic  mode, where Rail-RNA always compresses intermediate data.  This option leaves quality sequences intact.  Boolean parameter; has no argument.", 
            "title": "--do-not-bin-quals"
        }, 
        {
            "location": "/reference/#-short-read-names", 
            "text": "( prep  and  go  job flows only)  SAM/BAM file outputs are sometimes considerably larger simply because read names are large. This option kills read names in favor of short yet still unique identifiers.  Boolean parameter; has no argument.", 
            "title": "--short-read-names"
        }, 
        {
            "location": "/reference/#-skip-bad-records", 
            "text": "( prep  and  go  job flows only)  By default, if an input record from a FASTA/FASTQ file is invalid, Rail-RNA's preprocessing step chokes, as  reviewed in Tutorial . Use this parameter to instead be robust to bad records in input files and skip them. Bad records typically occur when an input file was somehow truncated or otherwise corrupted.  Boolean parameter; has no argument.", 
            "title": "--skip-bad-records"
        }, 
        {
            "location": "/reference/#-do-not-check-manifest", 
            "text": "( prep  and  go  job flows only)  Rail-RNA's default behavior is to check for the existence of each input file in the manifest file before starting a job flow. This can be time-consuming for a set of input files on a remote host. If you know the files are accessible,  --do-not-check-manifest  turns the default behavior off.  Boolean parameter; has no argument.", 
            "title": "--do-not-check-manifest"
        }, 
        {
            "location": "/reference/#-log-dir", 
            "text": "( local  and  parallel  modes only)  dir  is used both for aggregating intermediate files between steps of job flows and for storing log files for diagnosing problems.  Default: \"rail-rna_logs\" in the current working directory", 
            "title": "--log &lt;dir&gt;"
        }, 
        {
            "location": "/reference/#-intermediate-s3_dirhdfs_dir", 
            "text": "( elastic  mode only)  This is a directory in an S3 bucket or on the temporary HDFS of an Elastic MapReduce cluster where intermediate data across all workers is aggregated. If using S3,  s3_dir  should begin with  s3://some-bucket . If using HDFS,  hdfs_dir  should begin with  hdfs:/// . Use S3 and set  --intermediate-lifetime  to -1 to keep intermediates.  Default: argument of  -o/--output  with the string \".intermediate\" tacked on", 
            "title": "--intermediate &lt;s3_dir/hdfs_dir&gt;"
        }, 
        {
            "location": "/reference/#-p-num-processes-int", 
            "text": "( local  mode only)  Rail-RNA runs your job flow on up to  int  processes simultaneously.  Default: if you have more than one processing core available, Rail-RNA runs on as many as you have less one so it's not a resource hog. If you have only one processing core, Rail-RNA runs on it.", 
            "title": "-p/--num-processes &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-scratch-dir", 
            "text": "( local  and  parallel  modes only)  This is a directory used for storing temporary files accumulated when aggregating intermediate data. In  parallel  mode, this should always be in some node-local directory like  /tmp .  Default: securely created temporary directory created by Python's  tempfile.mkdtemp()", 
            "title": "--scratch &lt;dir&gt;"
        }, 
        {
            "location": "/reference/#-keep-intermediates", 
            "text": "( local  and  parallel  modes only)  Rail-RNA's default behavior is to delete aggregated intermediate data as soon as the job flow it's running no longer needs them. Keeping them around after a job flow is complete using  --keep-intermediates  is sometimes useful for diagnosing problems, however.  Boolean parameter; has no argument.", 
            "title": "--keep-intermediates"
        }, 
        {
            "location": "/reference/#-g-gzip-intermediates", 
            "text": "( local  and  parallel  modes only)  Rail-RNA has a small memory footprint but a large storage footprint. When space is tight, the storage footprint can be minimized in  local  and  parallel  modes by compressing intermediates with the  -g/--gzip-intermediates  command-line parameter. This will slow down the job flow a bit.  Boolean parameter; has no argument.", 
            "title": "-g/--gzip-intermediates"
        }, 
        {
            "location": "/reference/#-gzip-level-int", 
            "text": "( local  and  parallel  modes only)  This is the level of gzip compression to use for intermediate data if it's being compressed. The extremes are 1, meaning no compression, and 9, meaning high compression. Values closer to 9 increase the time a given job flow takes significantly.  Default: 3", 
            "title": "--gzip-level &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-r-sort-memory-cap-dec", 
            "text": "( local  and  parallel  modes only)  Rail-RNA uses  GNU Coreutils   sort  when aggregating intermediate data.  dec  is the maximum amount of memory (in bytes) used by  sort  per process  Default: 307200", 
            "title": "-r/--sort-memory-cap &lt;dec&gt;"
        }, 
        {
            "location": "/reference/#-nucleotides-per-input-int", 
            "text": "( local  and  parallel  modes only)  After the  prep  job flow downloads a sample's FASTA/FASTQs from a remote server, it assigns reads to alignment tasks. Each task may be executed by a different worker.  int  is the maximum number of nucleotides from input reads to assign to any single task. Smaller values of  int  increase parallelism in the following alignment step. This command-line parameter does not apply to input data stored locally, which Rail-RNA attempts to distribute evenly among workers.  Default: 100000000", 
            "title": "--nucleotides-per-input &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-do-not-gzip-input", 
            "text": "( local  and  parallel  modes only)  By default, Rail-RNA gzips preprocessed input data, which is  not  considered intermediate data. Turning this option on leaves preprocessed input reads uncompressed.", 
            "title": "--do-not-gzip-input"
        }, 
        {
            "location": "/reference/#-max-task-attempts-int", 
            "text": "Sometimes, a worker attempting a task will fail for miscellaneous reasons. For example, the node on which the worker is operating may transiently run out of space. Hadoop clusters are resilient to failures like this because they automatically reattempt tasks. Rail-RNA can reattempt tasks in  local  and  parallel  modes, too, when it's not using Hadoop.  --max-task-attempts  applies in all modes: a task is reattempted up to  int  times by different workers in the cluster, if possible, before failing a job flow. On Elastic MapReduce, which uses Hadoop,  int  specifies the config parameters  mapreduce.map.maxattempts  and  mapreduce.reduce.maxattempts .  Algorithm options  These are parameters that tweak Rail-RNA's algorithms.", 
            "title": "--max-task-attempts &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-bowtie2-args-str", 
            "text": "These are arguments to pass to Bowtie 2, and they should be enclosed in\ndouble-quotes. A couple arguments are always ignored, however.   Bowtie 2 is always run in  --local  mode by Rail-RNA, so  --end-to-end  won't be passed to it.  Rail-RNA bins quality scores in a particular way, and  --mp  arguments won't be passed to Bowtie 2.   Default: empty", 
            "title": "--bowtie2-args &lt;str&gt;"
        }, 
        {
            "location": "/reference/#-k-int", 
            "text": "Tells Rail-RNA to look for up to  int  alignments per read per step. This takes precedence over any  -k  value specified in --bowtie2-args.  Default: no value; Rail-RNA looks for the best alignment.", 
            "title": "-k &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-isofrag-idx-fx-file", 
            "text": "This is a  tar.gz file  containing the transcript fragment (isofrag) index from previous Rail-RNA run. It is used in lieu of searching for exon-exon junctions. You'll find an  isofrags.tar.gz  file with the isofrag index in the  cross_sample_outputs/  directory of a Rail-RNA run where  idx  wasone of the  deliverables , which is the default behavior.  Default: none; search for junctions.", 
            "title": "--isofrag-idx/-fx &lt;file&gt;"
        }, 
        {
            "location": "/reference/#-partition-length-int", 
            "text": "To compute genome coverage by reads, Rail-RNA partitions the genome into bins of length  int  bases. Each bin is assigned to a different task to be executed by a different worker. The exon differentials described in our  paper  are summed within each bin to arrive at base-level coverage.  Partition lengths that are too large decrease parallelism and contribute to load imbalance, but partition lengths that are too small (tens or hundreds of bases) make for larger intermediate data. It's probably not necessary to change the default value.  Default: 5000", 
            "title": "--partition-length &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-max-readlet-size-int", 
            "text": "Rail-RNA divides reads into short overlapping segments called readlets and subsequently aligns them to the genome to infer the presence of exon-exon junctions. For a complete description of the readletizing scheme Rail uses, see  our paper . A readlet's size does not exceed  int  bases.  For human-size genomes, values between 22 and 27 work well, with smaller values improving sensitivity but decreasing specificity.  Default: 25", 
            "title": "--max-readlet-size &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-min-readlet-size-int", 
            "text": "See  --max-readlet-size  for a description of readlets. Keep  int  above 10 to avoid making too many false positive exon-exon junction calls.  Default: 15", 
            "title": "--min-readlet-size &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-readlet-interval-int", 
            "text": "See  --max-readlet-size  for a description of readlets. Here,  int  is the number of bases between the start positions of consecutive overlapping readlets. Increasing this value decreases sensitivity to novel exon-exon junctions. However, we haven't found that decreasing  --readlet-interval  below 4 increaes sensitivity much.  Default: 4", 
            "title": "--readlet-interval &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-readlet-config-size-int", 
            "text": "After finding exon-exon junctions in samples, Rail-RNA constructs a directed acyclic graph (DAG) whose nodes represent introns. Each edge has weight equal to the number of exonic bases between the intron nodes it connects. Rail-RNA traverses the DAG to find all possible combinations of exon-exon junctions spanned by  int  exonic bases. It later aligns reads to transcript fragments spanning these combinations of exon-exon junctions.  We find a  --readlet-config-size  above 30 to be sufficient to resolve alignments spanning exon-exon junctions for human-size genomes.  Default: 35", 
            "title": "--readlet-config-size &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-max-intron-size-int", 
            "text": "This parameter has Rail filter out introns that span more than  int  bases. It significantly reduces false positive exon-exon junction calls. There are only a handful of introns that exceed 500,000 bases in the human genome, which explains the default.  Default: 500000", 
            "title": "--max-intron-size &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-min-intron-size-int", 
            "text": "This parameter has Rail filter out introns spanning fewer than  int  bases. Making this too small might have Rail mistake insertions for introns.  Default: 10", 
            "title": "--min-intron-size &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-min-exon-size-int", 
            "text": "This sets various parameters across Rail-RNA that enforce its sensitivity to exons spanning only at least  int  bases. Most conspicuously:   When searching for exon-exon junctions with readlet alignments, Rail-RNA will look for small exons that span at least  int  bases. Making  int  smaller increases false positive junction calls.  When traversing the DAG described under  --readlet-config-size , Rail-RNA will eliminate junction combinations spanning exons below  int .   Default: 9", 
            "title": "---min-exon-size &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-search-filter-choiceint", 
            "text": "Rail-RNA aligns all read sequences to the genome first with Bowtie 2 in  --local  mode, which can soft-clip base sequences at read ends. Ordinarily, all read sequences with soft-clipped bases are divided into readlets and later searched for exon-exon junctions---but instead, the read sequences searched for junctions may be chosen based on how many if its bases are soft-clipped in its primary alignment instead. If  choice/int  is an integer  int , a read sequence is only searched for junctions if  =  int  of its bases are soft-clipped.  choice/int  can also be  choice  from { strict ,  mild ,  none }. If  choice  is  strict ,  int  is set equal to  --min-exon-size ; if  choice  is  mild ,  int  is 2/3 *  --min-exon-size ; and if  choice  is  none ,  int  is 1.  Default: none; i.e., 1", 
            "title": "--search-filter &lt;choice/int&gt;"
        }, 
        {
            "location": "/reference/#-junction-criteria-decint", 
            "text": "When Rail-RNA analyzes more than one RNA-seq sample, it accumulates a master list of exon-exon junctions across samples. This master list is used to construct transcript fragments to which reads are realigned. However, when hundreds of samples are analyzed, the master list typically blows up: true positive junction calls tend to be shared by many samples, while false positive junction calls tend to be unique to small numbers of samples. As the number of samples analyzed increases, so does the master list of junctions, and false positive calls make up a larger and larger fraction of the master list. False positive calls may be due to low-quality reads or repetitive/small exonic segments on either side of an intron that make it difficult to resolve the intron's proper start and end positions in the genome.  The  --junction-criteria  parameter  dec,int  filters out junctions that are not either present in at least a fraction  dec  of samples or detected in at least  int  reads of one sample. This way, only junctions that are detected from a small number of reads in any one sample or from a small number of samples are eliminated, and the quality of realigned reads is improved without significantly compromising sensitivity. (See  our paper  for an elaboration of this phenomenon.) For example, specifying  --junction-criteria 0.02,7  or  --junction-criteria 0.02 7  filters out junctions that are found in fewer that 2% of samples and are detected in fewer in 7 reads in any one sample before realignment.  Default: 0.05,5", 
            "title": "--junction-criteria &lt;dec,int&gt;"
        }, 
        {
            "location": "/reference/#-normalize-percentile-dec", 
            "text": "Normalization factors for coverage of the genome by primary alignments and by uniquely aligning reads are output by Rail-RNA if the  tsv  deliverable is requested; see the  appropriate section of Deliverables . The normalization factor of a given sample can be obtained by   Counting the number n of bases that are covered by at least k reads, where k  = 1.  Writing a list of numbers where each value k occurs exactly n times.  Sorting the list constructed in step 2 in increasing order.  Selecting the  dec *100th-percentile value from the list.   When  dec  is 0.75, this is  upper-quartile normalization .  Default: 0.75", 
            "title": "--normalize-percentile &lt;dec&gt;"
        }, 
        {
            "location": "/reference/#-do-not-drop-polya-tails", 
            "text": "By default Rail-RNA, does not map reads for which all bases to the left or right of a segment spanning  --min-exon-size  bases   Boolean parameter; has no argument.", 
            "title": "--do-not-drop-polyA-tails"
        }, 
        {
            "location": "/reference/#-tie-margin-int", 
            "text": "Ordinarily, two alignments of a read are considered to have \"tied\" alignment scores if and only if the scores are exactly equal, and  int  is 0. However, for the purpose of identifying uniquely aligning reads and resolving primary alignments based on coverage, toggling this parameter permits a score difference of  int  per 100 bases among alignments whose scores are considered ties. For example, 150\nand 144 are tied alignment scores for a 100-bp read when --tie-margin is 6, and if a read has alignments with both scores, it is not considered uniquely aligned.  Default: 0", 
            "title": "--tie-margin &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-library-size-int", 
            "text": "As described in  Deliverables , in the  cross_sample_outputs  directory,  [mean|median]. chr .bw  encodes the [mean|median] coverage at each base across samples on chromosome  chr  by primary alignments, and  [mean|median]. chr .bw  encodes the [mean|median] coverage at each base on chromosome  chr  by uniquely aligning reads. To normalize each sample's contribution to a given [mean|median], the number of reads covering a given base of the genome is multiplied by  int *1,000,000 and divided by the number of mapped reads in the sample. So  int  is a standard library size in millions of reads that permits easier interpretation of the mean and median coverage bigWigs.  Default: 40  Output options", 
            "title": "--library-size &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-o-output-dir", 
            "text": "Rail-RNA will write its output to the directory  dir . In  local  mode,  dir  can be on either the local filesystem or on S3. In  parallel  mode,  dir  can be on S3 or at some path accessible to all nodes in your IPython cluster. In  elastic  mode,  dir  must be on S3.  Default:  ./rail-rna_out  in  local  and  parallel  modes; a required parameter in  elastic  mode", 
            "title": "-o/--output &lt;dir&gt;"
        }, 
        {
            "location": "/reference/#-d-deliverables-choice", 
            "text": "This command-line option permits suppressing some outputs of the full Rail-RNA pipeline. Check out  Deliverables  to learn more.", 
            "title": "-d/--deliverables &lt;choice,...&gt;"
        }, 
        {
            "location": "/reference/#-drop-deletions", 
            "text": "By default, Rail-RNA counts bases deleted from the reference in a read alignment as covered by the read when computing the coverage vectors stored in bigWigs. Turning this option adjusts Rail-RNA's behavior so such deleted bases do not contribute to the coverage vectors.  Boolean parameter; has no argument.", 
            "title": "--drop-deletions"
        }, 
        {
            "location": "/reference/#-do-not-output-bam-by-chr", 
            "text": "By default, Rail-RNA outputs alignment BAMs by chromosome/sample to increase parallelism in a terminal output step. You can output BAMs by sample alone with this option.  Boolean parameter; has no argument.", 
            "title": "--do-not-output-bam-by-chr"
        }, 
        {
            "location": "/reference/#-do-not-output-ave-bw-by-chr", 
            "text": "Obtaining a bigWig storing average (mean or median) coverage across many samples can involve much more computation than obtaining a bigWig storing coverage for a single sample. Moreover, an average coverage bigWigs can take up considerably more space than a single-sample bigWig. To mitigate the resulting load balance issues, Rail-RNA outputs average coverage bigWigs chromosome by chromosome, by default. You can output each average coverage bigWig for the entire genome instead with this option.  Boolean parameter; has no argument.", 
            "title": "--do-not-output-ave-bw-by-chr"
        }, 
        {
            "location": "/reference/#-indel-criteria-decint", 
            "text": "Accumulating indels across hundreds of samples bloats the  insertions.tsv.gz  and  deletions.tsv.gz  files with many false positives for which there is little evidence. The  --indel-criteria  parameter  dec,int  does not write indels to these cross-sample output files unless they are either present in at least a fraction  dec  of samples or detected in at least  int  reads of one sample.  --indel-criteria 's usage is similar to that of  --junction-criteria .  Default:  0.05,5  Elastic MapReduce options  These are arguments that pertain to working with Amazon Web Services in Rail-RNA's  elastic  mode.", 
            "title": "--indel-criteria &lt;dec,int&gt;"
        }, 
        {
            "location": "/reference/#-intermediate-lifetime-int", 
            "text": "It costs money to keep files around on Simple Storage Service (S3). To help save you money, Rail-RNA schedules any directory in which it stores intermediate data on S3 for deletion after  int  days. If  int  is set to  -1 , no such scheduling is done, and intermediate data is kept for indefinitely long.  Check  Lifecycle  under a given bucket's properties in the  S3 console  to view and adjust the rules Rail-RNA has made to delete intermediate data.  Default: 4", 
            "title": "--intermediate-lifetime &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-name-str", 
            "text": "It can be useful to assign a distinctive name  str  to an Elastic MapReduce job flow if you're working with many in the  console .  Default:  Rail-RNA Job Flow", 
            "title": "--name &lt;str&gt;"
        }, 
        {
            "location": "/reference/#-log-uri-s3_dir", 
            "text": "This is the directory on S3 to which Elastic MapReduce will ultimately copy Hadoop logs.  Default: argument of  -o/--output  with the string \".logs\" tacked on", 
            "title": "--log-uri &lt;s3_dir&gt;"
        }, 
        {
            "location": "/reference/#-ami-version-str", 
            "text": "This is the version of the Amazon Machine Image to load onto each node of the Elastic MapReduce cluster. Changing  str  is not recommended  Default:  3.8.0", 
            "title": "--ami-version &lt;str&gt;"
        }, 
        {
            "location": "/reference/#-visible-to-all-users", 
            "text": "This makes the Elastic MapReduce cluster accessible to all IAM users associated with the main Amazon Web Services account.  Boolean parameter; has no argument.", 
            "title": "--visible-to-all-users"
        }, 
        {
            "location": "/reference/#-action-on-failure-choice", 
            "text": "This is the action Elastic MapReduce should take if the job flow it's running fails on a given step.  choice  is one of { TERMINATE_JOB_FLOW ,  CANCEL_AND_WAIT ,  CONTINUE ,  TERMINATE_CLUSTER }.  Default:  TERMINATE_JOB_FLOW", 
            "title": "--action-on-failure &lt;choice&gt;"
        }, 
        {
            "location": "/reference/#-master-instance-count-int", 
            "text": "This is the number of master instances to include in your Elastic MapReduce cluster. It's only useful to include more than one in case a master instance dies; in general,  int  shouldn't be changed.  Default: 1", 
            "title": "--master-instance-count &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-task-instance-count-int", 
            "text": "Task instances are nodes in an Elastic MapReduce cluster that do not store data. Your job flow will continue even if they're lost, which can happen if nodes go down or--more likely--they're  spot instances  whose maximum bid price no longer exceeds the market prices. You should consider using task instances only if you'll be bidding for them on the spot market at the rate  --task-instance-bid-price .  Default: 0", 
            "title": "--task-instance-count &lt;int&gt;"
        }, 
        {
            "location": "/reference/#-master-instance-bid-price-dec", 
            "text": "If you'd like to use the  spot market  to potentially save money on your job flow, you can set the bid price (in dollars/hour) for the master instance group here. Invoke this command-line parameter only if master instances should be spot.  Default: none; use on-demand master instances", 
            "title": "--master-instance-bid-price &lt;dec&gt;"
        }, 
        {
            "location": "/reference/#-core-instance-bid-price-dec", 
            "text": "If you'd like to use the  spot market  to potentially save money on your job flow, you can set the bid price (in dollars/hour) for the core instance group here. Invoke this command-line parameter only if core instances should be spot.  Default: none; use on-demand core instances", 
            "title": "--core-instance-bid-price &lt;dec&gt;"
        }, 
        {
            "location": "/reference/#-task-instance-bid-price-dec", 
            "text": "If you'd like to use the  spot market  to potentially save money on your job flow, you can set the bid price (in dollars/hour) for the task instance group here (if you have any task instances). Invoke this command-line parameter only if you are using task instances. Indeed, there is typically no point to task instances unless they're spot instances: you may as well use on-demand core instances and be afforded extra space.", 
            "title": "--task-instance-bid-price &lt;dec&gt;"
        }, 
        {
            "location": "/reference/#-master-instance-type-choice", 
            "text": "This is the  instance type  of the master instance group of your Elastic MapReduce cluster.  Default:  c3.2xlarge", 
            "title": "--master-instance-type &lt;choice&gt;"
        }, 
        {
            "location": "/reference/#-core-instance-type-choice", 
            "text": "This is the  instance type  of the core instance group of your Elastic MapReduce cluster.  Default: argument of  --master-instance-type", 
            "title": "--core-instance-type &lt;choice&gt;"
        }, 
        {
            "location": "/reference/#-task-instance-type-choice", 
            "text": "This is the  instance type  of the core instance group of your Elastic MapReduce cluster.  Default: argument of  --master-instance-type  if there are any task instances", 
            "title": "--task-instance-type &lt;choice&gt;"
        }, 
        {
            "location": "/reference/#-ec2-key-name-str", 
            "text": "You can specify an Elastic Compute Cloud (EC2) key pair name  str  for SSHing to the master instance of your Elastic MapReduce cluster while your job flow is running.  Default: unspecified, which means you can't SSH to the master instance of your cluster", 
            "title": "--ec2-key-name &lt;str&gt;"
        }, 
        {
            "location": "/reference/#-keep-alive", 
            "text": "This option keeps an Elastic Mapreduce cluster alive when all its assigned steps are complete. Use  --keep-alive  in conjunction with  --termination-protected  and  --ec2-key-name  to be able to SSH to an Elastic MapReduce cluster and diagnose problems. However, you should make sure to terminate the cluster manually when you're finished.  Boolean parameter; has no argument.", 
            "title": "--keep-alive"
        }, 
        {
            "location": "/reference/#-termination-protected", 
            "text": "This option protects an Elastic MapReduce cluster from termination in case of step failure. Use  --termination-protected  in conjunction with  --keep-alive  and  --ec2-key-name  to be able to SSH an Elastic MapReduce cluster and diagnose problems. However, you should make sure to terminate the cluster manually when you're finished.", 
            "title": "--termination-protected"
        }, 
        {
            "location": "/reference/#-region-choice", 
            "text": "This specifies the region in which your job flow will be run. Valid regions are given  here .  Default: the region from your  --profile , but if that's unavailable,  us-east-1  (US Standard)", 
            "title": "--region &lt;choice&gt;"
        }, 
        {
            "location": "/reference/#-service-role-str", 
            "text": "You should have set up your IAM service role by entering  aws emr create-default-roles  after installing the AWS CLI. If for some reason related to permissioning you need to use a different IAM service role, specify its name as  str .  Default: taken from  --profile  if available; otherwise, attempts  EMR_DefaultRole", 
            "title": "--service-role &lt;str&gt;"
        }, 
        {
            "location": "/reference/#-instance-profile-str", 
            "text": "You should have set up your IAM EC2 instance profile by entering  aws emr create-default-roles  after installing the AWS CLI. If for some reason related to permissioning you need to use a different IAM EC2 instance profile, specify its name as  str .  Default: taken from  --profile  if available; otherwise, attempts  EMR_EC2_DefaultRole", 
            "title": "--instance-profile &lt;str&gt;"
        }
    ]
}